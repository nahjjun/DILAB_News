{"cells":[{"cell_type":"markdown","metadata":{"id":"YEhZ7slLqoY-"},"source":["# 1. 코랩 드라이브 연결"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18878,"status":"ok","timestamp":1754808855004,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"qd1nwRl6oPV4","outputId":"da0bf50e-14a9-4db4-ea55-1e205dac4b74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hnPBp85dqiL9"},"source":["# 2. 해당 셀의 코드 지정한 경로에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1754808960720,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"WM-UwvezvTuK","outputId":"cacb577e-d562-4152-d7a4-99fe8524d8dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/DILAB/HJ/News_DILAB/naver_news_cleaner.py\n"]}],"source":["%%writefile /content/drive/MyDrive/DILAB/HJ/News_DILAB/naver_news_cleaner.py\n","\n","import re\n","import json\n","import requests\n","import urllib.parse\n","import html\n","from bs4 import BeautifulSoup\n","\n","# ─── 설정 ───\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n","        \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n","        \"Mobile/15A5341f Safari/604.1\"\n","    )\n","}\n","\n","\n","\n","# ─── 1) 검색결과에서 페이지별로 데이터 뽑기 ───\n","def fetch_search_links(keyword: str, pages: int = 5):\n","    seen = set() # 중복된 기사 url 제거를 위한 set 자료형\n","    results = [] # 최종으로 반환할 (제목, url) 구조의 튜플 리스트\n","\n","    # 페이지 단위로 데이터 크롤링\n","    for page in range(1, pages+1):\n","        # 검색 결과의 시작 위치 인덱스 지정\n","        start = 1 + (page-1) * 10\n","\n","        # 네이버 모바일 뉴스 검색 결과 페이지를 구성하는 URL 쿼리 파라미터 지정\n","        url = (\n","            \"https://m.search.naver.com/search.naver\"\n","            \"?ssc=tab.m_news.all\" # 네이버 모바일의 검색 옵션을 틀었을 때 나오는 파라미터.\n","            f\"&query={urllib.parse.quote(keyword)}\"\n","            f\"&start={start}\"\n","            \"&pd=4\"  # 기간 조건을 활성화\n","            \"&nso=so:r,p:1d\"  # nso파라미터 최신순 정렬(nso=so:{정렬방식},p:{기간지정},a{기타옵션}) + 최근 1일\n","        )\n","\n","        # Naver모바일 뉴스 검색 결과 페이지의 HTML을 불러와서 BeautifulSoup객체로 파싱\n","          # get 요청 보낼 url, header, timeout 설정\n","          # timeout : 요청 제한 시간 설정(초 단위)\n","        resp = requests.get(url, headers=HEADERS, timeout=5)\n","        resp.raise_for_status() # raise_for_status() => Response가 오류일 경우, 예외를 발생시키는 함수\n","        soup = BeautifulSoup(resp.text, \"html.parser\") # response로 온 text 본문(html 코드) 'html.parser' 타입 지정 후 BeautifulSoup 객체 생성\n","\n","\t\t\t\t# 링크<a href=\"...\">요소만 골라냄\n","        for a in soup.find_all(\"a\", href=True):\n","            # a[\"href\"]로 URL을 / a.get_text(strip=True)로 태그 안의 보이는 텍스트(제목)을 가져옴\n","\t\t\t\t    # strip=True는 양쪽 공백을 자동으로 제거\n","            href, text = a[\"href\"], a.get_text(strip=True)\n","\n","            # 링크 URL이 Naver뉴스 도메인인지, 그리고 제목에 키워드가 포함되었는지 확인\n","            if href.startswith(\"https://n.news.naver.com/article/\") and keyword in text:\n","                if href not in seen:\n","                    seen.add(href)\n","                    results.append((text, href))\n","    return results\n","\n","# ─── 2) URL → 모바일 읽기 페이지 → 본문 추출 ───\n","# 입력을 url이라는 stirng형식을 받아서, 결과로 string으로 반환하는 함수를 정의\n","def fetch_article_body(url: str) -> str:   # -> 파이썬의 함수 어노테이션 함수. 함수의 인자, 반환값의 타입 권장을 지정할 수 있음\n","    # 1) 데스크탑용 URL → 모바일 읽기용 URL로 변환\n","    # 데스크탑(n.news.naver.com) -> 모바일(m.news.naver.com)형식으로 변환\n","    m = re.match(r\"https://n\\.news\\.naver\\.com/article/(\\d+)/(\\d+)\", url)\n","        # re.match(pattern, string) => 지정한 패턴이 string의 시작부터 맞는지 확인하는 함수\n","        # https://n.news.naver.com/article/ + 숫자가 1번 이상 반복 + \"/\" + 숫자가 한번 이상 반복이라는 정규식\n","        # Match 객체를 반환하는데, 해당 객체로 patter과 string이 일치한 부분들을 가져올 수 있다.\n","\n","    # 만약 받은 문자열이 해당 모바일 읽기용 url이 맞다면, 해당 기사의 ID를 추출해서 모바일 url로 재구성한다.\n","    if m:\n","        # 기사 ID(oid, aid)를 추출해서 모바일 URL로 재구성\n","        # oid : 언론사 ID / aid : 기사 ID\n","        oid, aid = m.groups()\n","        url = f\"https://m.news.naver.com/read.nhn?oid={oid}&aid={aid}\"\n","        # ㄴ> read.nhn : 네이버 뉴스 모바일 웹의 \"기사 보기 뷰 핸들러(legacy 엔드포인트)\"\n","\n","    # 2) 페이지 요청 & 파싱\n","    # requests.get()으로 해당 URL페이지 HTML을 가져옴, HEADERS는 브라우저처럼 보이기 위한\n","    # User-Agent 포함헤더\n","    resp = requests.get(url, headers=HEADERS, timeout=5)\n","    # 요청 실패시 에러 발생\n","    resp.raise_for_status()\n","    # BeautifulSoup으로 HTML파싱\n","    soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","    # 3) 사용 가능한 본문 컨테이너를 가져오기\n","    container = (\n","        soup.select_one(\"div#newsct_article\")\n","        or soup.select_one(\"div#articleBodyContents\") # 위에거가 없으면 이거라도\n","        or soup.select_one(\"div#newsEndContents\") # 위에거가 없으면 이거라도\n","    )\n","    if not container:\n","        return \"\"\n","    # ㄴ> 여기서 container는 Tag 객체이다.\n","\n","\n","    # 4) 컨테이너 전체 텍스트를 줄 단위로 가져오기\n","    # HTML태그를 제거하고 순수 텍스트만 추출\n","    # 문단 구분을 위해 \\n으로 나눔\n","    raw = container.get_text(separator=\"\\n\", strip=True)\n","\n","\n","    # 5) 줄별 필터링: 짧거나 안내 문구 제거\n","    # 길이가 30자 미만인 줄은 대부분 광고나 잡다한 문구이므로 제외\n","    # bad_tokens에 포함된 단어가 있는 줄도 제외\n","    bad_tokens = [\"구독\", \"언론사\", \"댓글\", \"프리미엄\", \"beta\"]\n","    lines = []\n","    # .splitlines() 해당 텍스트를 줄바꿈 기준으로 나눠서 리스트 형태로 반환\n","    for line in raw.splitlines():\n","        txt = line.strip() # 공백 제거\n","        if len(txt) < 30:\n","            continue\n","\n","        # bad_tokens에서 token을 하나씩 빼서 확인하는데, 해당 token이 txt에 들어있는지(in) 확인한다.\n","        # 만약 해당 경우(전체 조건) 중 하나라도 True이면(any) => 전체 결과가 True이다.\n","        if any(token in txt for token in bad_tokens):\n","            continue\n","        # bad_tokens에 포함되는 단어가 들어가지 않은 txt라면, return할 lines 배열에 결과 append\n","        lines.append(txt)\n","\n","    # 6) 빈 줄 한 줄씩 넣어서 합치기\n","    # 문단 사이에 빈 줄을 넣어서 가독성 향상\n","    return \"\\n\\n\".join(lines)\n","\n","\n","\n","# 3) 정제 (Cleaning)\n","\n","def clean_text(text):\n","    # 1) HTML 엔티티(특수문자) 디코딩\n","    t = html.unescape(text)\n","\n","    # 2) 스마트따옴표·전각문자 통일\n","    t = t.replace('“', '\"').replace('”', '\"')\n","\n","    # 3) 이메일·URL·숫자 플레이스홀더\n","    # re.X 플래그를 지정해줌으로써, 패턴 내 공백, 주석 허용\n","\n","    t = re.sub(r'''\n","        \\b  # 단어 경계 (앵커)\n","        [\\w\\.-]+ # (문자들(A-Za-z0-9), \".\", \"-\") 중 하나가 1회 이상 반복됨\n","        @[\\w\\.-]+ # 위와 동일\n","        \\.\\w+ # \".\" 다음에 단어가 1개 이상 반복됨\n","        \\b  # 단어 경계 종료\n","    ''', '<EMAIL>', t, re.X) # 해당 형태의 단어 \"<EMAIL>\" 태그로 변경\n","\n","\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', t)\n","\n","    # 4) 불필요 기호 제거\n","    t = re.sub(r'[※■▶★♡♥]', '', t)\n","    # ㄴ> t에서 해당 기호들 중 하나에 해당하는 부분이 있다면 ''로 치환\n","\n","    # 5) 괄호 속 설명 제거\n","    t = re.sub(r'''\n","        \\[        # 대괄호 열기\n","        [^\\]]*    # [...] 중 하나 & \"]\"가 아닌 문자들이 0개 이상\n","        \\]        # 대괄호 닫기\n","        |         # or\n","        \\(        # 소괄호 열기\n","        [^\\)]*    # [...] 중 하나 & \")\"가 아닌 문자들이 0개 이상\n","        \\)        # 소괄호 닫기\n","    ''', '', t, flags=re.X)\n","\n","    # 6) 제어문자 제거\n","    t = re.sub(r'[\\t\\r]', ' ', t) # \\t, \\r 중 하나\n","\n","    # 7) 추가 정규화\n","    t = re.sub(r'\\b네이버뉴스\\b', '', t)\n","    t = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', t)\n","    t = re.sub(r'(사진=?|=사진|/사진)', '', t)\n","\n","    # 8) 연속 공백/개행 통일\n","    t = re.sub(r'\\n{3,}', '\\n\\n', t) # \\n이 3번 이상 반복하면 \\n\\n으로 치환\n","    t = re.sub(r' {2,}', ' ', t) # 공백이 2번 이상 반복하면 공백 1칸으로 치환\n","\n","    # 9) strip (공백 제거)\n","    return t.strip()\n","\n","\n","\n","\n","# 몇개의 데이터가 저장되었는지 반환한다.\n","def save_articles_to_jsonl(keyword: str, output_path: str, pages: int = 5):\n","    \"\"\"\n","      (이전: TXT 저장) → (변경: JSONL 저장)\n","      한 줄에 하나의 JSON 객체를 기록: {\"title\": \"...\", \"body\": \"...\"}\n","      Hugging Face datasets.load_dataset(\"json\", data_files=..., split=\"train\")로 바로 로드 가능.\n","      반환값: 실제로 기록된 레코드 수\n","    \"\"\"\n","    links = fetch_search_links(keyword, pages) # 제공받은 키워드, 페이지 수로 기사 제목, 링크 가져옴\n","\n","    written = 0\n","    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","      for title, href in links:\n","        try:\n","          body = fetch_article_body(href)\n","        except Exception:# 개별 기사 parsing 실패는 건너뛰도록 설정\n","          continue\n","\n","        clean_title = clean_text(title)\n","        clean_body = clean_text(body)\n","\n","        j = {\"title\": clean_title, \"body\": clean_body}\n","\n","        f.write(json.dumps(j, ensure_ascii=False))\n","          # dumps : 인자로 넘겨준 파이썬 객체(딕셔너리, 리스트 등)를 json 문자열로 변환해주는 함수\n","          # ensure_ascii를 False로 설정하면 비 아스키 문자도 원형 그대로 저장해줌 (기본값은 True)\n","        f.write(\"\\n\")\n","        written += 1\n","\n","    return written\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nPkGPt-SqaON"},"source":["# naver_news_cleaner.py 파일 사용 예제\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50237,"status":"ok","timestamp":1754809024098,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"zItFm1BWqZYa","outputId":"2f5f2d14-c3c7-4ec2-9018-43793e1a7606"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["36"]},"metadata":{},"execution_count":5}],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/DILAB/HJ/News_DILAB') # 모듈이 있는 경로 지정\n","from naver_news_cleaner import save_articles_to_jsonl\n","\n","save_articles_to_jsonl('IT', \"/content/drive/MyDrive/DILAB/HJ/News_DILAB/news_articles.jsonl\", 10)"]},{"cell_type":"markdown","source":["# 구글 Colab 데이터 크롤링 스케줄러 설정"],"metadata":{"id":"5_ML5XrzIXtU"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/DILAB/HJ/News_DILAB/main.py\n","\n","from datetime import datetime\n","import os\n","from naver_news_cleaner import save_articles_to_jsonl\n","\n","def run_daily_news_crawler(keyword: str, base_dir: str = \"./news_data\", pages: int = 6):\n","    # 오늘 날짜 형식\n","    today = datetime.now().strftime(\"%Y-%m-%d\")\n","    filename = f\"news_articles_{today}.jsonl\"\n","    os.makedirs(base_dir, exist_ok=True)\n","    output_path = os.path.join(base_dir, filename)\n","\n","    count = save_articles_to_jsonl(keyword, output_path, pages=pages)\n","    print(f\"[{today}] '{keyword}' 관련 뉴스 {count}건 저장 완료 → {output_path}\")\n","\n","if __name__ == \"__main__\":\n","    run_daily_news_crawler(\"IT\", pages=6)\n"],"metadata":{"id":"_G1B43JKIXis","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754809242682,"user_tz":-540,"elapsed":378,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"2cc3c7ca-5696-4cb6-b9b4-8e7fcec0a148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/DILAB/HJ/News_DILAB/main.py\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqXoAtIORpF33mEf2zHZWZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}