{"cells":[{"cell_type":"markdown","metadata":{"id":"YEhZ7slLqoY-"},"source":["# 1. 코랩 드라이브 연결"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10491,"status":"ok","timestamp":1753892663831,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"qd1nwRl6oPV4","outputId":"9b1c73b8-5cfd-4a3a-c8ca-8056ee1ae629"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Collecting schedule\n","  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n","Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n","Installing collected packages: schedule\n","Successfully installed schedule-1.2.2\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hnPBp85dqiL9"},"source":["# 2. 해당 셀의 코드 지정한 경로에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1753891596394,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"WM-UwvezvTuK","outputId":"7584b931-e631-4926-bfac-59f32836c690"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/DILAB/HJ/News_DILAB/naver_news_cleaner.py\n"]}],"source":["%%writefile /content/drive/MyDrive/DILAB/HJ/News_DILAB/naver_news_cleaner.py\n","\n","import re\n","import requests\n","import urllib.parse\n","import html\n","from bs4 import BeautifulSoup\n","\n","# ─── 설정 ───\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n","        \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n","        \"Mobile/15A5341f Safari/604.1\"\n","    )\n","}\n","\n","\n","\n","# ─── 1) 검색결과에서 페이지별로 데이터 뽑기 ───\n","def fetch_search_links(keyword: str, pages: int = 5):\n","    seen = set() # 중복된 기사 url 제거를 위한 set 자료형\n","    results = [] # 최종으로 반환할 (제목, url) 구조의 튜플 리스트\n","\n","    # 페이지 단위로 데이터 크롤링\n","    for page in range(1, pages+1):\n","        # 검색 결과의 시작 위치 인덱스 지정\n","        start = 1 + (page-1) * 10\n","\n","        # 네이버 모바일 뉴스 검색 결과 페이지를 구성하는 URL 쿼리 파라미터 지정\n","        url = (\n","            \"https://m.search.naver.com/search.naver\"\n","            \"?ssc=tab.m_news.all\" # 네이버 모바일의 검색 옵션을 틀었을 때 나오는 파라미터.\n","            f\"&query={urllib.parse.quote(keyword)}\"\n","            f\"&start={start}\"\n","            \"&pd=4\"  # 기간 조건을 활성화\n","            \"&nso=so:r,p:1d\"  # nso파라미터 최신순 정렬(nso=so:{정렬방식},p:{기간지정},a{기타옵션}) + 최근 1일\n","        )\n","\n","        # Naver모바일 뉴스 검색 결과 페이지의 HTML을 불러와서 BeautifulSoup객체로 파싱\n","          # get 요청 보낼 url, header, timeout 설정\n","          # timeout : 요청 제한 시간 설정(초 단위)\n","        resp = requests.get(url, headers=HEADERS, timeout=5)\n","        resp.raise_for_status() # raise_for_status() => Response가 오류일 경우, 예외를 발생시키는 함수\n","        soup = BeautifulSoup(resp.text, \"html.parser\") # response로 온 text 본문(html 코드) 'html.parser' 타입 지정 후 BeautifulSoup 객체 생성\n","\n","\t\t\t\t# 링크<a href=\"...\">요소만 골라냄\n","        for a in soup.find_all(\"a\", href=True):\n","            # a[\"href\"]로 URL을 / a.get_text(strip=True)로 태그 안의 보이는 텍스트(제목)을 가져옴\n","\t\t\t\t    # strip=True는 양쪽 공백을 자동으로 제거\n","            href, text = a[\"href\"], a.get_text(strip=True)\n","\n","            # 링크 URL이 Naver뉴스 도메인인지, 그리고 제목에 키워드가 포함되었는지 확인\n","            if href.startswith(\"https://n.news.naver.com/article/\") and keyword in text:\n","                if href not in seen:\n","                    seen.add(href)\n","                    results.append((text, href))\n","    return results\n","\n","# ─── 2) URL → 모바일 읽기 페이지 → 본문 추출 ───\n","# 입력을 url이라는 stirng형식을 받아서, 결과로 string으로 반환하는 함수를 정의\n","def fetch_article_body(url: str) -> str:   # -> 파이썬의 함수 어노테이션 함수. 함수의 인자, 반환값의 타입 권장을 지정할 수 있음\n","    # 1) 데스크탑용 URL → 모바일 읽기용 URL로 변환\n","    # 데스크탑(n.news.naver.com) -> 모바일(m.news.naver.com)형식으로 변환\n","    m = re.match(r\"https://n\\.news\\.naver\\.com/article/(\\d+)/(\\d+)\", url)\n","        # re.match(pattern, string) => 지정한 패턴이 string의 시작부터 맞는지 확인하는 함수\n","        # https://n.news.naver.com/article/ + 숫자가 1번 이상 반복 + \"/\" + 숫자가 한번 이상 반복이라는 정규식\n","        # Match 객체를 반환하는데, 해당 객체로 patter과 string이 일치한 부분들을 가져올 수 있다.\n","\n","    # 만약 받은 문자열이 해당 모바일 읽기용 url이 맞다면, 해당 기사의 ID를 추출해서 모바일 url로 재구성한다.\n","    if m:\n","        # 기사 ID(oid, aid)를 추출해서 모바일 URL로 재구성\n","        # oid : 언론사 ID / aid : 기사 ID\n","        oid, aid = m.groups()\n","        url = f\"https://m.news.naver.com/read.nhn?oid={oid}&aid={aid}\"\n","        # ㄴ> read.nhn : 네이버 뉴스 모바일 웹의 \"기사 보기 뷰 핸들러(legacy 엔드포인트)\"\n","\n","    # 2) 페이지 요청 & 파싱\n","    # requests.get()으로 해당 URL페이지 HTML을 가져옴, HEADERS는 브라우저처럼 보이기 위한\n","    # User-Agent 포함헤더\n","    resp = requests.get(url, headers=HEADERS, timeout=5)\n","    # 요청 실패시 에러 발생\n","    resp.raise_for_status()\n","    # BeautifulSoup으로 HTML파싱\n","    soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","    # 3) 사용 가능한 본문 컨테이너를 가져오기\n","    container = (\n","        soup.select_one(\"div#newsct_article\")\n","        or soup.select_one(\"div#articleBodyContents\") # 위에거가 없으면 이거라도\n","        or soup.select_one(\"div#newsEndContents\") # 위에거가 없으면 이거라도\n","    )\n","    if not container:\n","        return \"\"\n","    # ㄴ> 여기서 container는 Tag 객체이다.\n","\n","\n","    # 4) 컨테이너 전체 텍스트를 줄 단위로 가져오기\n","    # HTML태그를 제거하고 순수 텍스트만 추출\n","    # 문단 구분을 위해 \\n으로 나눔\n","    raw = container.get_text(separator=\"\\n\", strip=True)\n","\n","\n","    # 5) 줄별 필터링: 짧거나 안내 문구 제거\n","    # 길이가 30자 미만인 줄은 대부분 광고나 잡다한 문구이므로 제외\n","    # bad_tokens에 포함된 단어가 있는 줄도 제외\n","    bad_tokens = [\"구독\", \"언론사\", \"댓글\", \"프리미엄\", \"beta\"]\n","    lines = []\n","    # .splitlines() 해당 텍스트를 줄바꿈 기준으로 나눠서 리스트 형태로 반환\n","    for line in raw.splitlines():\n","        txt = line.strip() # 공백 제거\n","        if len(txt) < 30:\n","            continue\n","\n","        # bad_tokens에서 token을 하나씩 빼서 확인하는데, 해당 token이 txt에 들어있는지(in) 확인한다.\n","        # 만약 해당 경우(전체 조건) 중 하나라도 True이면(any) => 전체 결과가 True이다.\n","        if any(token in txt for token in bad_tokens):\n","            continue\n","        # bad_tokens에 포함되는 단어가 들어가지 않은 txt라면, return할 lines 배열에 결과 append\n","        lines.append(txt)\n","\n","    # 6) 빈 줄 한 줄씩 넣어서 합치기\n","    # 문단 사이에 빈 줄을 넣어서 가독성 향상\n","    return \"\\n\\n\".join(lines)\n","\n","\n","\n","# 3) 정제 (Cleaning)\n","\n","def clean_text(text):\n","    # 1) HTML 엔티티(특수문자) 디코딩\n","    t = html.unescape(text)\n","\n","    # 2) 스마트따옴표·전각문자 통일\n","    t = t.replace('“', '\"').replace('”', '\"')\n","\n","    # 3) 이메일·URL·숫자 플레이스홀더\n","    # re.X 플래그를 지정해줌으로써, 패턴 내 공백, 주석 허용\n","\n","    t = re.sub(r'''\n","        \\b  # 단어 경계 (앵커)\n","        [\\w\\.-]+ # (문자들(A-Za-z0-9), \".\", \"-\") 중 하나가 1회 이상 반복됨\n","        @[\\w\\.-]+ # 위와 동일\n","        \\.\\w+ # \".\" 다음에 단어가 1개 이상 반복됨\n","        \\b  # 단어 경계 종료\n","    ''', '<EMAIL>', t, re.X) # 해당 형태의 단어 \"<EMAIL>\" 태그로 변경\n","\n","\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', t)\n","\n","    # 4) 불필요 기호 제거\n","    t = re.sub(r'[※■▶★♡♥]', '', t)\n","    # ㄴ> t에서 해당 기호들 중 하나에 해당하는 부분이 있다면 ''로 치환\n","\n","    # 5) 괄호 속 설명 제거\n","    t = re.sub(r'''\n","        \\[        # 대괄호 열기\n","        [^\\]]*    # [...] 중 하나 & \"]\"가 아닌 문자들이 0개 이상\n","        \\]        # 대괄호 닫기\n","        |         # or\n","        \\(        # 소괄호 열기\n","        [^\\)]*    # [...] 중 하나 & \")\"가 아닌 문자들이 0개 이상\n","        \\)        # 소괄호 닫기\n","    ''', '', t, flags=re.X)\n","\n","    # 6) 제어문자 제거\n","    t = re.sub(r'[\\t\\r]', ' ', t) # \\t, \\r 중 하나\n","\n","    # 7) 연속 공백/개행 통일\n","    t = re.sub(r'\\n{3,}', '\\n\\n', t) # \\n이 3번 이상 반복하면 \\n\\n으로 치환\n","    t = re.sub(r' {2,}', ' ', t) # 공백이 2번 이상 반복하면 공백 1칸으로 치환\n","\n","    # 8) strip (공백 제거)\n","    return t.strip()\n","\n","\n","\n","\n","\n","def save_articles_to_txt(keyword: str, output_path: str, pages: int = 5):\n","    links = fetch_search_links(keyword, pages) # 제공받은 키워드, 페이지 수로 기사 제목, 링크 가져옴\n","    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","        for i, (title, href) in enumerate(links, 1): # 몇번째 기사인지 붙여주기 위해 enumerate 함수로 각 요소에 순번 부여\n","            body = fetch_article_body(href)\n","            clean_title = clean_text(title)\n","            clean_body = clean_text(body)\n","\n","            f.write(f\"### 기사 {i}\\n\")\n","            f.write(clean_title + \"\\n\\n\")\n","            f.write(clean_body + \"\\n\")\n","            f.write(\"=\" * 80 + \"\\n\\n\")\n","    return len(links)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nPkGPt-SqaON"},"source":["# naver_news_cleaner.py 파일 사용 예제\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30718,"status":"ok","timestamp":1753891687583,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"zItFm1BWqZYa","outputId":"0a4e21a7-f543-4178-f945-d09fcaadff1f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["31"]},"metadata":{},"execution_count":4}],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/DILAB/HJ/News_DILAB') # 모듈이 있는 경로 지정\n","from naver_news_cleaner import save_articles_to_txt\n","\n","save_articles_to_txt('IT', \"/content/drive/MyDrive/DILAB/HJ/News_DILAB/news_articles.txt\", 10)"]},{"cell_type":"markdown","source":["# 구글 Colab 데이터 크롤링 스케줄러 설정"],"metadata":{"id":"5_ML5XrzIXtU"}},{"cell_type":"code","source":["# main.py\n","from datetime import datetime\n","import os\n","from naver_news_cleaner import save_articles_to_txt\n","\n","def run_daily_news_crawler(keyword: str, base_dir: str = \"./news_data\", pages: int = 6):\n","    # 오늘 날짜 형식\n","    today = datetime.now().strftime(\"%Y-%m-%d\")\n","    filename = f\"news_articles_{today}.txt\"\n","    os.makedirs(base_dir, exist_ok=True)\n","    output_path = os.path.join(base_dir, filename)\n","\n","    count = save_articles_to_txt(keyword, output_path, pages=pages)\n","    print(f\"[{today}] '{keyword}' 관련 뉴스 {count}건 저장 완료 → {output_path}\")\n","\n","if __name__ == \"__main__\":\n","    run_daily_news_crawler(\"IT\", pages=6)\n"],"metadata":{"id":"_G1B43JKIXis"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPr5rqKvRwsetGOdJlUGLLE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}