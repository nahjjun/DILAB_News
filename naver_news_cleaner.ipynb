{"cells":[{"cell_type":"markdown","metadata":{"id":"YEhZ7slLqoY-"},"source":["# 1. 코랩 드라이브 연결"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2397,"status":"ok","timestamp":1753356799714,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"qd1nwRl6oPV4","outputId":"0ac87395-e223-4cca-82ee-e1d0514fa4f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hnPBp85dqiL9"},"source":["# 2. 해당 셀의 코드 지정한 경로에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89,"status":"ok","timestamp":1753356874572,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"WM-UwvezvTuK","outputId":"a68306f7-ca4c-40eb-9750-f8204f12dbd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/DILAB/HJ/News_DILAB/naver_news_cleaner.py\n"]}],"source":["import re\n","import requests\n","import urllib.parse\n","import html\n","from bs4 import BeautifulSoup\n","\n","# ─── 설정 ───\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n","        \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n","        \"Mobile/15A5341f Safari/604.1\"\n","    )\n","}\n","\n","\n","\n","# ─── 1) 검색결과에서 페이지별로 데이터 뽑기 ───\n","def fetch_search_links(keyword: str, pages: int = 5):\n","    seen = set() # 중복된 기사 url 제거를 위한 set 자료형\n","    results = [] # 최종으로 반환할 (제목, url) 구조의 튜플 리스트\n","\n","    # 페이지 단위로 데이터 크롤링\n","    for page in range(1, pages+1):\n","        # 검색 결과의 시작 위치 인덱스 지정\n","        start = 1 + (page-1) * 10\n","\n","        # 네이버 모바일 뉴스 검색 결과 페이지를 구성하는 URL 쿼리 파라미터 지정\n","        url = (\n","            \"https://m.search.naver.com/search.naver\"\n","            \"?where=m_news\"\n","            f\"&query={urllib.parse.quote(keyword)}\"\n","            f\"&start={start}\"\n","        )\n","\n","        # Naver모바일 뉴스 검색 결과 페이지의 HTML을 불러와서 BeautifulSoup객체로 파싱\n","          # get 요청 보낼 url, header, timeout 설정\n","          # timeout : 요청 제한 시간 설정(초 단위)\n","        resp = requests.get(url, headers=HEADERS, timeout=5)\n","        resp.raise_for_status() # raise_for_status() => Response가 오류일 경우, 예외를 발생시키는 함수\n","        soup = BeautifulSoup(resp.text, \"html.parser\") # response로 온 text 본문(html 코드) 'html.parser' 타입 지정 후 BeautifulSoup 객체 생성\n","\n","\t\t\t\t# 링크<a href=\"...\">요소만 골라냄\n","        for a in soup.find_all(\"a\", href=True):\n","            # a[\"href\"]로 URL을 / a.get_text(strip=True)로 태그 안의 보이는 텍스트(제목)을 가져옴\n","\t\t\t\t    # strip=True는 양쪽 공백을 자동으로 제거\n","            href, text = a[\"href\"], a.get_text(strip=True)\n","\n","            # 링크 URL이 Naver뉴스 도메인인지, 그리고 제목에 키워드가 포함되었는지 확인\n","            if href.startswith(\"https://n.news.naver.com/article/\") and keyword in text:\n","                if href not in seen:\n","                    seen.add(href)\n","                    results.append((text, href))\n","    return results\n","\n","# ─── 2) URL → 모바일 읽기 페이지 → 본문 추출 ───\n","# 입력을 url이라는 stirng형식을 받아서, 결과로 string으로 반환하는 함수를 정의\n","def fetch_article_body(url: str) -> str:\n","    # 1) 데스크탑용 URL → 모바일 읽기용 URL로 변환\n","    # 데스크탑(n.news.naver.com) -> 모바일(m.news.naver.com)형식으로 변환\n","    m = re.match(r\"https://n\\.news\\.naver\\.com/article/(\\d+)/(\\d+)\", url)\n","    if m:\n","        # 기사 ID(oid, aid)를 추출해서 모바일 URL로 재구성\n","        oid, aid = m.groups()\n","        url = f\"https://m.news.naver.com/read.nhn?oid={oid}&aid={aid}\"\n","\n","    # 2) 페이지 요청 & 파싱\n","    # requests.get()으로 해당 URL페이지 HTML을 가져옴, HEADERS는 브라우저처럼 보이기 위한\n","    # User-Agent 포함헤더\n","    resp = requests.get(url, headers=HEADERS, timeout=5)\n","    # 요청 실패시 에러 발생\n","    resp.raise_for_status()\n","    # BeautifulSoup으로 HTML파싱\n","    soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","    # 3) 가능한 본문 컨테이너들\n","    container = (\n","        soup.select_one(\"div#newsct_article\")\n","        or soup.select_one(\"div#articleBodyContents\")\n","        or soup.select_one(\"div#newsEndContents\")\n","    )\n","    if not container:\n","        return \"\"\n","\n","    # 4) 컨테이너 전체 텍스트를 줄 단위로 가져오기\n","    # HTML태그를 제거하고 순수 텍스트만 추출\n","    # 문단 구분을 위해 \\n으로 나눔\n","    raw = container.get_text(separator=\"\\n\", strip=True)\n","    bad_tokens = [\"구독\", \"언론사\", \"댓글\", \"프리미엄\", \"beta\"]\n","    lines = []\n","\n","    # 5) 줄별 필터링: 짧거나 안내 문구 제거\n","    # 길이가 30자 미만인 줄은 대부분 광고나 잡다한 문구이므로 제외\n","    # bad_tokens에 포함된 단어가 있는 줄도 제외\n","    for line in raw.splitlines():\n","        txt = line.strip()\n","        if len(txt) < 30:\n","            continue\n","        if any(token in txt for token in bad_tokens):\n","            continue\n","        lines.append(txt)\n","\n","    # 6) 빈 줄 한 줄씩 넣어서 합치기\n","    # 문단 사이에 빈 줄을 넣어서 가독성 향상\n","    return \"\\n\\n\".join(lines)\n","\n","\n","\n","# 3) 정제 (Cleaning)\n","\n","def clean_text(text):\n","    # 1) HTML 엔티티(특수문자) 디코딩\n","    t = html.unescape(text)\n","\n","    # 2) 스마트따옴표·전각문자 통일\n","    t = t.replace('“', '\"').replace('”', '\"')\n","\n","    # 3) 이메일·URL·숫자 플레이스홀더\n","    # re.X 플래그를 지정해줌으로써, 패턴 내 공백, 주석 허용\n","\n","    t = re.sub(r'''\n","        \\b  # 단어 경계 (앵커)\n","        [\\w\\.-]+ # (문자들(A-Za-z0-9), \".\", \"-\") 중 하나가 1회 이상 반복됨\n","        @[\\w\\.-]+ # 위와 동일\n","        \\.\\w+ # \".\" 다음에 단어가 1개 이상 반복됨\n","        \\b  # 단어 경계 종료\n","    ''', '<EMAIL>', t, re.X) # 해당 형태의 단어 \"<EMAIL>\" 태그로 변경\n","\n","\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', t)\n","\n","    # 4) 불필요 기호 제거\n","    t = re.sub(r'[※■▶★♡♥]', '', t)\n","    # 5) 괄호 속 설명 제거\n","    t = re.sub(r'\\[[^\\]]*\\]|\\([^)]*\\)', '', t)\n","    # 6) 제어문자 제거\n","    t = re.sub(r'[\\t\\r]', ' ', t)\n","    # 7) 연속 공백/개행 통일\n","    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n","    t = re.sub(r' {2,}', ' ', t)\n","    # 8) strip\n","    return t.strip()\n","\n","\n","\n","\n","\n","def save_articles_to_txt(keyword: str, output_path: str, pages: int = 5):\n","    links = fetch_search_links(keyword, pages)\n","    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n","        for i, (title, href) in enumerate(links, 1):\n","            body = fetch_article_body(href)\n","            clean_title = clean_text(title)\n","            clean_body = clean_text(body)\n","\n","            f.write(f\"### 기사 {i}\\n\")\n","            f.write(clean_title + \"\\n\\n\")\n","            f.write(clean_body + \"\\n\")\n","            f.write(\"=\" * 80 + \"\\n\\n\")\n","    return len(links)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nPkGPt-SqaON"},"source":["# naver_news_cleaner.py 파일 사용 예제\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18530,"status":"ok","timestamp":1753356897935,"user":{"displayName":"Di Lab","userId":"16690045528125560783"},"user_tz":-540},"id":"zItFm1BWqZYa","outputId":"e7e106b0-2ee7-4118-99b9-367f9d3db6c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":17}],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/DILAB/HJ/News_DILAB') # 모듈이 있는 경로 지정\n","from naver_news_cleaner import save_articles_to_txt\n","\n","save_articles_to_txt('IT', \"/content/drive/MyDrive/DILAB/HJ/News_DILAB/news_articles.txt\", 1)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyqJZPiBsvMyuH1s47c4qp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}