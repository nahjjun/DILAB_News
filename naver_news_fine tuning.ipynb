{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNkSe7iHVjFkw4rfBVmLney"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3e85fd7004e146ab8f234c962fd95722":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d622ce410ce14ff5a155e25649408fcc","IPY_MODEL_3741372b83e042ce80fd5cf2cbe3cbee","IPY_MODEL_7a385743b77f436c94ba81441066e16a"],"layout":"IPY_MODEL_625efc98ea3348a79fb8d96996b8112c"}},"d622ce410ce14ff5a155e25649408fcc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80b48f6cb66d43959438e6533bc547a3","placeholder":"​","style":"IPY_MODEL_8f0d06cd701b428a8fbe74e6918be645","value":"Loading checkpoint shards: 100%"}},"3741372b83e042ce80fd5cf2cbe3cbee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d51b40cf965d4da1aa0209e08e00b11b","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e8a7c5440914d0fb8ad79f79532dc5b","value":4}},"7a385743b77f436c94ba81441066e16a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c6b40148e7c4b768079d37211eb24eb","placeholder":"​","style":"IPY_MODEL_009f5d65c26a45938e5a33b35ae49a92","value":" 4/4 [03:40&lt;00:00, 45.93s/it]"}},"625efc98ea3348a79fb8d96996b8112c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80b48f6cb66d43959438e6533bc547a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f0d06cd701b428a8fbe74e6918be645":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d51b40cf965d4da1aa0209e08e00b11b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8a7c5440914d0fb8ad79f79532dc5b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c6b40148e7c4b768079d37211eb24eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"009f5d65c26a45938e5a33b35ae49a92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa61eae904914e9da5358169c571f9cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aecf9c45d6bd4690815a194e4e5525ab","IPY_MODEL_955385e0ab7f4b1d97c1430bbea203d8","IPY_MODEL_2aa45dd2663440d39b5006bff753fbce"],"layout":"IPY_MODEL_81a8b180194f4afc9f9a1b9bca244f32"}},"aecf9c45d6bd4690815a194e4e5525ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0a10a937fd240f4b653b141595b3689","placeholder":"​","style":"IPY_MODEL_2ea88a0c711c4f7cb9eb34462a6f0bd6","value":"Epoch 1:  12%"}},"955385e0ab7f4b1d97c1430bbea203d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_35c153cec02e406096d9d4eff5b7b554","max":140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6be7daf8c3594c35ab3462c56e19590c","value":17}},"2aa45dd2663440d39b5006bff753fbce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db202372e0204acfbb9d7a35c75a84c2","placeholder":"​","style":"IPY_MODEL_0cb07f3e0b054673aa15d9505c3c27b8","value":" 17/140 [01:15&lt;08:28,  4.14s/it, loss=2.36]"}},"81a8b180194f4afc9f9a1b9bca244f32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0a10a937fd240f4b653b141595b3689":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea88a0c711c4f7cb9eb34462a6f0bd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c153cec02e406096d9d4eff5b7b554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be7daf8c3594c35ab3462c56e19590c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db202372e0204acfbb9d7a35c75a84c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cb07f3e0b054673aa15d9505c3c27b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 0. 드라이브 - 코랩 연결"],"metadata":{"id":"db5RLF7jPjQE"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWv_qmB7MW4d","executionInfo":{"status":"ok","timestamp":1755708097150,"user_tz":-540,"elapsed":20486,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"6fb2bdf6-ac2e-452f-b935-0c717d1b036f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -U bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbHwjM80pGi0","executionInfo":{"status":"ok","timestamp":1755708106809,"user_tz":-540,"elapsed":9655,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"3542af7b-3e55-4fb5-de0d-73f3dba69095"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.47.0\n"]}]},{"cell_type":"markdown","source":["# 1. 토크나이즈된 .arrow 데이터 불러오기"],"metadata":{"id":"Wxe3B0aXPqrg"}},{"cell_type":"code","source":["from datasets import load_from_disk\n","\n","train_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/train_tokenized')\n","valid_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/valid_tokenized')\n","\n","print(len(train_ds[\"input_ids\"]))\n","print(valid_ds)\n","\n","# \"input_ids\" : 토크나이저가 텍스트를 분해해 만든 토큰 정수 ID 배열\n","# \"attention_mask\" : 어떤 토큰을 실제로 계산에 반영할지 표시하는 마스크\n","  # ㄴ> 1: 유효 토큰, 0: 패딩(무시)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EY_tK9vyPrGw","executionInfo":{"status":"ok","timestamp":1755708115324,"user_tz":-540,"elapsed":8511,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"1bd4512f-e086-498a-a33e-a5a266f7020e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["280\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 32\n","})\n"]}]},{"cell_type":"markdown","source":["# 2. labels(학습을 위한 정답지) 생성"],"metadata":{"id":"Y78F2P0PS0hY"}},{"cell_type":"code","source":["IGNORE_INDEX = -100 # CE Loss의 ignore_index 값은 -100이다.\n","\n","# 해당 train/valid 데이터셋에 각각의 labels를 추가해주는 함수\n","def add_labels_batched(batch):\n","  ids_batch = batch[\"input_ids\"]\n","  # attention_mask를 가져오는데, 만약 해당 값이 없다면 default 값으로 input_ids 길이만큼의 \"1\"로 채워진 batch형 배열 반환\n","  am_batch = batch.get(\"attention_mask\", [[1]*len(ids) for ids in ids_batch])\n","\n","  # attention_mask에서 1이 아닌 자리는 IGNORE_INDEX로 채우고, 1인 자리는 input_ids의 값을 복사한다.\n","  # 해당 작업을 배치 단위로 진행한다.\n","  labels_batch = [\n","      [tok if m==1 else IGNORE_INDEX for tok, m in zip(ids, am)]\n","      for ids, am in zip(ids_batch, am_batch)\n","  ]\n","  return {\"labels\": labels_batch}\n","\n","# 각각의 데이터셋에 \"labels\"를 추가함\n","  # num_proc : 병렬 처리에 사용할 프로세스 개수\n","  # batched : 해당 map작업을 배치 단위로 하는지 여부 설정\n","# ㄴ> map 함수는 해당 함수 작업을 수행한 결과를 해당 데이터셋에 추가한 값을 반환한다.\n","train_ds = train_ds.map(add_labels_batched, batched=True, num_proc=4)\n","valid_ds = valid_ds.map(add_labels_batched, batched=True, num_proc=4)\n","\n","\n","# 해당 데이터셋에는 \"text\" 컬럼이 있는데, 해당 컬럼은 학습 과정에서 필요 없기 때문에 제거한다\n","train_ds = train_ds.remove_columns(\"text\")\n","valid_ds = valid_ds.remove_columns(\"text\")"],"metadata":{"id":"Ml3mZa6tS0v4","executionInfo":{"status":"ok","timestamp":1755708121508,"user_tz":-540,"elapsed":6181,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 3. 모델에 8비트 양자화 + LoRA 적용"],"metadata":{"id":"2df0b72w1RF-"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, default_data_collator\n","from transformers import AutoTokenizer  # 임베딩 리사이즈 & 생성 시에만 사용\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","model_path = \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n","# PAD 토큰이 없다면, EOS 토큰으로 해당 역할 대체\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","# 1. 8비트 양자화 로드\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit = True,\n","    # fp16 대신 bfloat16을 연산 타입으로 사용 (bfloat16이 fp16보다 안정적임)\n","    bnb_8bit_compute_dtype = torch.bfloat16\n",")\n","\n","# 2. 8비트 양자화 적용한 모델 로드\n","# AutoModelForCausalLM으로 8비트 양자화를 적용하여 저장해둔 모델(라마 3)을 로드한다.\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    quantization_config = bnb_config, # 모델을 8-bit 양자화 하여 GPU 메모리를 절약 (Hugging Face + bitsandbytes 라이브러리로 작동함)\n","    device_map = \"auto\", # \"auto\": 모델을 사용 가능한 장치에 자동으로 분산해서 올려준다 (GPU, CPU에 자동 분산) / \"cuda' : 자동 분산하지 않고 단일 GPU 사용\n","    torch_dtype = torch.bfloat16 # torch의 데이터 타입을 bfloat16으로 바꿔줌\n",")\n","\n","# 3.base_model의 입력 임베딩 테이블 resize\n","  # naver_news_tokeniziner 코드에서 텍스트에 \"<title></title>, <body></body>\" 스페셜 토큰을 적용해준 뒤 Tokenizer에도\n","  # 해당 스페셜 토큰들을 추가해주었었기 때문에 vocab(해당 Tokenizer의 어휘 사전 목록 길이)가 커졌다.\n","  # 따라서, 모델의 임베딩 테이블의 크기 또한 새로운 vocab으로 설정해주지 않으면 학습 중에 해당 토큰 id를 임베딩에서 찾을\n","  # 때 IndexError(범위 초과)가 날 수 있다.\n","    # ㄴ> 토크나이징 결과 숫자는 해당 토큰의 ID(Tokenizer, Model의 어휘 사전(vocab)에서의 위치)이다.\n","    # ㄴ> 해당 토큰 ID로 임베딩 테이블의 vector값을 찾는 것이다. (이 벡터값이 학습 중에 업데이트되는 가중치이다.)\n","    # ㄴ> 그렇기 때문에, resize_token_embedding()을 해주지 않으면, 임베딩 테이블에서 indexError가 날 수 있는 것이다.\n","base_model.resize_token_embeddings(len(tokenizer)) # base_model의 임베딩 행 개수를 늘려, 새 토큰들용 임베딩을 추가해야됨\n","  # ㄴ> model.resize_token_embeddings() : 모델의 입력 임베딩 테이블을 새 vocab 크기에 맞게 리사이즈하는 함수\n","    # 늘어나는 행 => 랜덤 초기화. 학습 중에 업데이트 된다\n","    # 줄이는 행 => 뒤쪽 행을 잘라냄\n","\n","# 4. K/V 캐시(past_key_values) 사용 여부를 끈다\n","  # 학습 중에는 gradient checkpointing과 충돌/경고가 나거나, 캐시 유지가 메모리를 더 먹을 수 있음\n","  # 따라서, 보통 학습할 때는 K/V 옵션을 끄며, 추론&생성 시에는 킨다\n","  # 효과 : 학습 중 경고 제거, 메모리 절약\n","base_model.config.use_cache = False\n","\n","# 5. Gradient(Activation) Checkpointing을 킨다\n"," # Gradient(Activation) Checkpointing : 순전파 때 중간 활성값을 전부 저장하지 않고, 역전파 때 일부를 재계산해서 VRAM을 크게 절약하는 기법\n"," # 장점: 메모리 사용량 감소 / 단점 : 연산량 증가\n"," # PyTorch 2.5부터는 gradient checkpointing에서 use_reentrant를 명시해야한다(안하면 경고 뜸)\n","base_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n","\n","\n","# 6. LoRA 적용\n","\n","  # 6-1) 양자화된 모델을 LoRA 학습이 가능한 상태로 설정\n","# prepare_model_for_kbit_training(model) : 8비트/4비트 양자화된 모델을 LoRA로 학습 가능한 상태로 안전하게 세팅해주는 함수\n","# LoRA를 모델에 붙이기 전에 해당 작업을 해주는 것이 바람직함\n","model_ready = prepare_model_for_kbit_training(base_model)\n","\n","  # 6-2) LoRA를 적용할 선형층 지정\n","# \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\" => 어텐션 모듈의 Q/K/V/O 투영 선형층\n","# \"gate_proj\",\"up_proj\",\"down_proj\" => FFN(MLP)의 게이트/상향/하향 선형층\n","target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]  # LLaMA3 계열 흔한 선택\n","\n","  # 6-3) LoRA 설정값 지정 (LoraConfig)\n","# LoraConfig() : LoRA 방식의 설정값들을 지정하는 함수\n","peft_conf = LoraConfig(\n","    r=16, # -> 랭크(학습할 저랭크 행렬의 차원 수). 클수록 표현력&파라미터 수 증가, VRAM/연산 증가 (보통 8~64 범위로 지정함)\n","    lora_alpha=32,  # -> scaling factor로, 최종 결과는 \"lora_alpha / r\"로 스케일링 됨(모델 성능에 영향을 줄 수 있음)\n","                    # -> 앞서 만든 \"A @ B\" 저랭크 행렬이 원래 weight W에 비해 너무 작은 값이 될 수 있으므로, 결과에 스케일링 계수(lora_alpha/r)를 곱해서 크기를 조절해주는 것.\n","    lora_dropout=0.05, # -> 과적합 방지를 위한 드롭아웃 확률 설정\n","    bias=\"none\", # -> 기존 모델의 bias(편향) 파라미터를 학습에 포함시킬 지 여부 지정 (\"lora_only\", \"all\" 옵션도 존재함)\n","    task_type=\"CAUSAL_LM\", # -> 작업 종류(저장/로드/일부 내부 로직에서 적절한 타입을 )\n","    target_modules=target_modules # -> 어떤 선형층에 LoRA를 주입할지 선택\n",")\n","\n","  # 6-4) LoRA 모듈 주입\n","# model_ready의 지정된 target_modules 선형층마다 \"LoRA 어댑터(저랭크 A/B 행렬)\"가 붙는다\n","# get_peft_model() : LoRA 설정에 맞게 기존 모델의 일부 모듈(q_proj, v_proj 등)에 LoRA 계층을 삽입해서, 전체 모델을 래핑(wrap)해준다.\n","  # ㄴ> LoRA가 주입된 PeftModel 객체를 반환한다.\n","model = get_peft_model(model_ready, peft_conf)\n"],"metadata":{"id":"l7d_t_QDH-fO","colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["3e85fd7004e146ab8f234c962fd95722","d622ce410ce14ff5a155e25649408fcc","3741372b83e042ce80fd5cf2cbe3cbee","7a385743b77f436c94ba81441066e16a","625efc98ea3348a79fb8d96996b8112c","80b48f6cb66d43959438e6533bc547a3","8f0d06cd701b428a8fbe74e6918be645","d51b40cf965d4da1aa0209e08e00b11b","3e8a7c5440914d0fb8ad79f79532dc5b","6c6b40148e7c4b768079d37211eb24eb","009f5d65c26a45938e5a33b35ae49a92"]},"executionInfo":{"status":"ok","timestamp":1755708365956,"user_tz":-540,"elapsed":244444,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"35bfa05c-b424-4038-efd4-37acc31479aa"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e85fd7004e146ab8f234c962fd95722"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"]},{"output_type":"execute_result","data":{"text/plain":["Embedding(128260, 4096)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# 4. N 에폭만큼 학습 진행"],"metadata":{"id":"5Wf1EqqwfI7Q"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","import torch.nn.utils as nn_utils\n","from contextlib import nullcontext\n","from tqdm.auto import tqdm\n","import math\n","\n","# 1. 배치 단위로 데이터 공급해주기 위해 DataLoader 사용\n","def collate_fn(batch):\n","    return {\n","        \"input_ids\":      torch.tensor([b[\"input_ids\"]      for b in batch], dtype=torch.long),\n","        \"attention_mask\": torch.tensor([b[\"attention_mask\"] for b in batch], dtype=torch.long),\n","        \"labels\":         torch.tensor([b[\"labels\"]         for b in batch], dtype=torch.long),\n","    }\n","\n","# 미니 배치들을 만드는 과정이다. 한 번의 iteration마다 샘플을 batch_size만큼 개를 꺼내온다는 뜻\n","# 데이터셋의 각 컬럼은 [X * Y]처럼 생겼는데, 해당 데이터에서 batch_size만큼의 행(샘플)들을 꺼내와서 배치화한다는 것이다.\n","train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  collate_fn=collate_fn, pin_memory=True)\n","valid_loader = DataLoader(valid_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n","\n","\n","\n","# 2. Optimizer 설정(Peft 사용 시, base_model의 trainable 파라미터만 학습된다.)\n","  # 옵티마이저는 기울기(미분값)를 사용하여 파라미터를 업데이트하는 역할을 한다.\n","\n","# optimizer = AdamW(model.parameters(), lr=2e-4) # -> Full 파인튜닝(모든 가중치 학습)을 할 때\n","  # ㄴ> Pytorch의 AdamW는 grad가 없는 파라미터는 step()에서 건너뛰지만, 불필요한 파라미터 레퍼런스를 옵티마이저가 들고 있게 되므로, 오버헤드가 증가하게 된다.\n","\n","optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=2e-4)\n","  # ㄴ> LoRA는 본체 가중치는 동결하고 LoRA 어댑터만 학습하기 때문에, 현재 model의 파라미터 중 학습할 파라미터만 모아서 옵티마이저에게 넘겨주어야 한다\n","  # ㄴ> requires_grad 속성으로 해당 파라미터가 학습할 파라미터인지 아닌지 확인한다.\n","\n","\n","# 3. GPU 사용 준비\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# 4. bf16 autocast (지원 GPU면 on하도록 하는 설정)\n","use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8  # Ampere+\n","amp_ctx = (torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16) if use_bf16 else nullcontext())\n","  # ㄴ> torch.autocast() 함수가 return한 객체는 \"컨텍스트 매니저 객체\"이다.\n","  # ㄴ> with와 함께 사용 시, 블록 내 연산을 \"자동 혼합 정밀(AMP)\"로 돌려서 속도&VRAM을 절약할 수 있다,\n","  # ㄴ> autocast 미지원 시, \"nullcontext()\"로 아무 것도 하지 않는 컨텍스트(fp32)로 동작한다.\n","\n","\n","# 5. 에폭 설정\n","epochs = 1\n","grad_accum = 8  # 몇개의 미니 배치가 모이면 optimizer로 한번에 기울기 업데이트를 할지 정하는 변수\n","\n","\n","# 6. 학습 시작\n","global_step = 0\n","for epoch in range(epochs):\n","  # 해당 모델을 \"학습 모드\"로 설정\n","  model.train()\n","\n","  total_loss = 0\n","\n","  # tqdm : 진행 상황을 bar로 보여주는 라이브러리\n","  running = 0.0\n","  loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\") # 데이터를 batch 단위로 반환해주는 Dataloader로 설정한 train_loader를 설정해준다.\n","        #  ㄴ> tqdm은 원래 iterable인 DataLoader를 감싼 iterable 객체이므로, \"for batch in tqdm_obj\"로 처럼 batch를 꺼내올 수 있다.\n","\n","  optimizer.zero_grad(set_to_none=True)\n","\n","  # DataLoader가 미니 배치로 나누어서 데이터들을 준다.\n","  # GPU의 한계 때문에 모든 데이터를 한번에 올리지 않고, 여러 개의 미니 배치로 데이터를 나눈 후 gradient 누적을 통해\n","  # 모든 데이터를 한번에 계산한 것과 같은(비슷한) 효과를 내도록 하는 것\n","    # grad_accum은 몇개의 배치를 묶어서 한 번 업데이트할 지를 지정하는 변수\n","  for step, batch in enumerate(loop, start=1): # enumerate는 매 반복마다 (인덱스, 값) 쌍을 뱉기 때문에 step, batch로 모은다\n","      # ㄴ> step : 만들어질 수 있는 총 배치의 개수\n","      batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n","        # ㄴ> 딕셔너리 컴프리헨션 -> {키식: 값식 for k, v in ...}\n","        # ㄴ> 기존 batch 딕셔너리의 모든 entry 쌍을 돌면서 값을 device(GPU)로 이동시킨다.\n","\n","      # autocast가 return한 AMP 컨텍스트 객체로 아래 블록의 작업을 수행한다.\n","      with amp_ctx:\n","          # out : Transformers의 \"ModelOutput\" 데이터 클래스\n","          out = model(**batch)           # CausalLM: loss 포함\n","                  # ㄴ> 딕셔너리 언패킹 : batch 내부의 여러 컬럼들\n","                  # {\"input_ids\" : X, \"attention_mask\" : Y, \"labels\" : Z}를\n","                  # model(input_ids=X, attention_mask=Y, labels=Z)로 키워드 인자로 풀어 넣는다.\n","          loss = out.loss / grad_accum\n","            # ㄴ> 그라디언트 누적을 위해, 각 배치의 loss값을 grad_accum으로 나눈 뒤에, 해당 loss값을 이용하여 역전파를 수행한다\n","            # ㄴ> loss값을 grad_accum으로 나누는 이유는 N개의 미니 배치의 평균 기울기를 만들기 위해서이다.\n","            # ㄴ> 해당 과정을 N번 누적하면, 평균 gradient가 된다.\n","\n","            # 손실함수란?\n","              # - 모델의 예측이 얼마나 틀렸는지를 숫자 하나(스칼라)로 측정하는 함수\n","              # - 해당 값을 최소화 하도록 경사 하강한다.\n","\n","      loss.backward()\n","        # ㄴ> .backward()를 호출하면 gradient가 이전의 gradient값(.grad)과 내부적으로 더해진다.\n","        # ㄴ> .backward()를 호출하는 순간, AutoGrad가 그래프를 거꾸로 타고 각 파라미터 tensor(leaf)의 param.grad에 기울기를 저장/누적함\n","\n","\n","      # 미리 설정한 배치 개수만큼 배치들이 모였다면, 그만큼 기울기가 축적되었다는 뜻이다.\n","      if step % grad_accum == 0:\n","          nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            # ㄴ> clip_grad_norm() : 기울기 폭주 방지(그래디언트 클리핑)\n","            # ㄴ> 더 자세히 찾아보기\n","\n","          optimizer.step()\n","            # ㄴ> 옵티마이저가 현재 .grad값으로 파라미터를 업데이트한다.\n","\n","          optimizer.zero_grad(set_to_none=True)\n","            # ㄴ> 다음 grad를 누적시키기 위해, 현재 설정되어있는 .grad값을 비운다.\n","            # ㄴ> set_to_none : 값을 0이 아닌 None으로 설정해서 메모리&속도 이점을 얻기 위한 속성\n","\n","          global_step += 1 # 전체 step 증가\n","\n","      # loss값을 grad_accum으로 나누었었기 때문에, N개 기준으로 맞춰진 것이 아니라 원래 스케일의 손실 평균을 로그에서 보여주기 위해 복구한다.\n","      running += loss.item() * grad_accum\n","      # step과 grad_accum이 동일해지면\n","      if step % grad_accum == 0:\n","          loop.set_postfix(loss=running/grad_accum) # tqdm의 진행바 오른쪽에 작은 딕셔너리를 붙여서 실시간 지표를 보여주는 함수\n","          running = 0.0\n","\n","\n"],"metadata":{"id":"vz3j7ojtfMbM","executionInfo":{"status":"error","timestamp":1755708441305,"user_tz":-540,"elapsed":75331,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"colab":{"base_uri":"https://localhost:8080/","height":537,"referenced_widgets":["fa61eae904914e9da5358169c571f9cf","aecf9c45d6bd4690815a194e4e5525ab","955385e0ab7f4b1d97c1430bbea203d8","2aa45dd2663440d39b5006bff753fbce","81a8b180194f4afc9f9a1b9bca244f32","a0a10a937fd240f4b653b141595b3689","2ea88a0c711c4f7cb9eb34462a6f0bd6","35c153cec02e406096d9d4eff5b7b554","6be7daf8c3594c35ab3462c56e19590c","db202372e0204acfbb9d7a35c75a84c2","0cb07f3e0b054673aa15d9505c3c27b8"]},"outputId":"f6cf3f0d-d414-433f-fdf2-f6c638aca50d"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["Epoch 1:   0%|          | 0/140 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa61eae904914e9da5358169c571f9cf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-980840848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m               \u001b[0;31m# - 해당 값을 최소화 하도록 경사 하강한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# ㄴ> .backward()를 호출하면 gradient가 이전의 gradient값(.grad)과 내부적으로 더해진다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# ㄴ> .backward()를 호출하는 순간, AutoGrad가 그래프를 거꾸로 타고 각 파라미터 tensor(leaf)의 param.grad에 기울기를 저장/누적함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# 5. 학습 끝난 PeftModel LoRA Adapter 드라이버에 저장"],"metadata":{"id":"vKC51XoUJKMl"}},{"cell_type":"code","source":["# DDP/Accelerate 사용 시엔 unwrap 필수!\n","# getattr(obj, name, default) : 객체의 속성 값을 안전하게 꺼내는 파이썬 내장 함수\n","peft_model = getattr(model, \"module\", model)\n","\n","peft_model.save_pretrained(\n","    \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B/lora_adapter\",\n","    safe_serialization=True  # adapter_model.safetensors 형식으로 저장하겠다는 의미\n","                    # ㄴ> 해당 값이 False이면, pytorch_model.bin(pickle 기반)으로 저장됨\n",")"],"metadata":{"id":"WtbllR4zJKhg","executionInfo":{"status":"aborted","timestamp":1755708441308,"user_tz":-540,"elapsed":1,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}}},"execution_count":null,"outputs":[]}]}