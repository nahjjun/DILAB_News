{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6/g+EefeeGHQLqwIoKIp4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 0. 드라이브 - 코랩 연결"],"metadata":{"id":"db5RLF7jPjQE"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWv_qmB7MW4d","executionInfo":{"status":"ok","timestamp":1755421793994,"user_tz":-540,"elapsed":18016,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"4e0bc51f-4bcf-467d-9124-bb0893581a85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 1. 토크나이즈된 .arrow 데이터 불러오기"],"metadata":{"id":"Wxe3B0aXPqrg"}},{"cell_type":"code","source":["from datasets import load_from_disk\n","\n","train_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/train_tokenized')\n","valid_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/valid_tokenized')\n","\n","print(train_ds)\n","print(valid_ds)\n","\n","# \"input_ids\" : 토크나이저가 텍스트를 분해해 만든 토큰 정수 ID 배열\n","# \"attention_mask\" : 어떤 토큰을 실제로 계산에 반영할지 표시하는 마스크\n","  # ㄴ> 1: 유효 토큰, 0: 패딩(무시)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EY_tK9vyPrGw","executionInfo":{"status":"ok","timestamp":1755421800128,"user_tz":-540,"elapsed":4891,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"4a4cb482-4ec7-4a44-fef3-dc87814f0475"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 280\n","})\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 32\n","})\n"]}]},{"cell_type":"markdown","source":["# 2. labels(학습을 위한 정답지) 생성"],"metadata":{"id":"Y78F2P0PS0hY"}},{"cell_type":"code","source":["IGNORE_INDEX = -100 # CE Loss의 ignore_index 값은 -100이다.\n","\n","# 해당 train/valid 데이터셋에 각각의 labels를 추가해주는 함수\n","def add_labels_batched(batch):\n","  ids_batch = batch[\"input_ids\"]\n","  # attention_mask를 가져오는데, 만약 해당 값이 없다면 default 값으로 input_ids 길이만큼의 \"1\"로 채워진 batch형 배열 반환\n","  am_batch = batch.get(\"attention_mask\", [[1]*len(ids) for ids in ids_batch])\n","\n","  # attention_mask에서 1이 아닌 자리는 IGNORE_INDEX로 채우고, 1인 자리는 input_ids의 값을 복사한다.\n","  # 해당 작업을 배치 단위로 진행한다.\n","  labels_batch = [\n","      [tok if m==1 else IGNORE_INDEX for tok, m in zip(ids, am)]\n","      for ids, am in zip(ids_batch, am_batch)\n","  ]\n","  return {\"labels\": labels_batch}\n","\n","# 각각의 데이터셋에 \"labels\"를 추가함\n","  # num_proc : 병렬 처리에 사용할 프로세스 개수\n","  # batched : 해당 map작업을 배치 단위로 하는지 여부 설정\n","# ㄴ> map 함수는 해당 함수 작업을 수행한 결과를 해당 데이터셋에 추가한 값을 반환한다.\n","train_ds = train_ds.map(add_labels_batched, batched=True, num_proc=4)\n","valid_ds = valid_ds.map(add_labels_batched, batched=True, num_proc=4)\n"],"metadata":{"id":"Ml3mZa6tS0v4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 모델에 8비트 양자화 + LoRA 적용"],"metadata":{"id":"2df0b72w1RF-"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, default_data_collator\n","from transformers import AutoTokenizer  # 임베딩 리사이즈 & 생성 시에만 사용\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","model_path = \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B\"\n","\n","# 1. 8비트 양자화 로드\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit = True,\n","    # fp16 대신 bfloat16을 연산 타입으로 사용 (bfloat16이 fp16보다 안정적임)\n","    bnb_8bit_compute_dtype = torch.bfloat16\n",")\n","\n","# 2. 8비트 양자화 적용한 모델 로드\n","# AutoModelForCausalLM으로 8비트 양자화를 적용하여 저장해둔 모델(라마 3)을 로드한다.\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    quantization_config = bnb_config, # 모델을 8-bit 양자화 하여 GPU 메모리를 절약 (Hugging Face + bitsandbytes 라이브러리로 작동함)\n","    device_map = \"auto\", # \"auto\": 모델을 사용 가능한 장치에 자동으로 분산해서 올려준다 (GPU, CPU에 자동 분산) / \"cuda' : 자동 분산하지 않고 단일 GPU 사용\n","    torch_dtype = torch.bfloat16 # torch의 데이터 타입을 bfloat16으로 바꿔줌\n",")\n","\n","# 3.base_model의 입력 임베딩 테이블 resize\n","  # naver_news_tokeniziner 코드에서 텍스트에 \"<title></title>, <body></body>\" 스페셜 토큰을 적용해준 뒤 Tokenizer에도\n","  # 해당 스페셜 토큰들을 추가해주었었기 때문에 vocab(해당 Tokenizer의 어휘 사전 목록 길이)가 커졌다.\n","  # 따라서, 모델의 임베딩 테이블의 크기 또한 새로운 vocab으로 설정해주지 않으면 학습 중에 해당 토큰 id를 임베딩에서 찾을\n","  # 때 IndexError(범위 초과)가 날 수 있다.\n","    # ㄴ> 토크나이징 결과 숫자는 해당 토큰의 ID(Tokenizer, Model의 어휘 사전(vocab)에서의 위치)이다.\n","    # ㄴ> 해당 토큰 ID로 임베딩 테이블의 vector값을 찾는 것이다. (이 벡터값이 학습 중에 업데이트되는 가중치이다.)\n","    # ㄴ> 그렇기 때문에, resize_token_embedding()을 해주지 않으면, 임베딩 테이블에서 indexError가 날 수 있는 것이다.\n","base_model.resize_token_embeddings(len(tokenizer)) # base_model의 임베딩 행 개수를 늘려, 새 토큰들용 임베딩을 추가해야됨\n","  # ㄴ> model.resize_token_embeddings() : 모델의 입력 임베딩 테이블을 새 vocab 크기에 맞게 리사이즈하는 함수\n","    # 늘어나는 행 => 랜덤 초기화. 학습 중에 업데이트 된다\n","    # 줄이는 행 => 뒤쪽 행을 잘라냄\n","\n","# 4. K/V 캐시(past_key_values) 사용 여부를 끈다\n","  # 학습 중에는 gradient checkpointing과 충돌/경고가 나거나, 캐시 유지가 메모리를 더 먹을 수 있음\n","  # 따라서, 보통 학습할 때는 K/V 옵션을 끄며, 추론&생성 시에는 킨다\n","  # 효과 : 학습 중 경고 제거, 메모리 절약\n","base_model.config.use_cache = False\n","\n","# 5. Gradient(Activation) Checkpointing을 킨다\n"," # Gradient(Activation) Checkpointing : 순전파 때 중간 활성값을 전부 저장하지 않고, 역전파 때 일부를 재계산해서 VRAM을 크게 절약하는 기법\n"," # 장점: 메모리 사용량 감소 / 단점 : 연산량 증가\n","base_model.gradient_checkpointing_enable()\n","\n","\n","# 6. LoRA 적용\n","\n","  # 6-1) 양자화된 모델을 LoRA 학습이 가능한 상태로 설정\n","# prepare_model_for_kbit_training(model) : 8비트/4비트 양자화된 모델을 LoRA로 학습 가능한 상태로 안전하게 세팅해주는 함수\n","# LoRA를 모델에 붙이기 전에 해당 작업을 해주는 것이 바람직함\n","model_ready = prepare_model_for_kbit_training(base_model)\n","\n","  # 6-2) LoRA를 적용할 선형층 지정\n","# \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\" => 어텐션 모듈의 Q/K/V/O 투영 선형층\n","# \"gate_proj\",\"up_proj\",\"down_proj\" => FFN(MLP)의 게이트/상향/하향 선형층\n","target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]  # LLaMA3 계열 흔한 선택\n","\n","  # 6-3) LoRA 설정값 지정 (LoraConfig)\n","# LoraConfig() : LoRA 방식의 설정값들을 지정하는 함수\n","peft_conf = LoraConfig(\n","    r=16, # -> 랭크(학습할 저랭크 행렬의 차원 수). 클수록 표현력&파라미터 수 증가, VRAM/연산 증가 (보통 8~64 범위로 지정함)\n","    lora_alpha=32,  # -> scaling factor로, 최종 결과는 \"lora_alpha / r\"로 스케일링 됨(모델 성능에 영향을 줄 수 있음)\n","                    # -> 앞서 만든 \"A @ B\" 저랭크 행렬이 원래 weight W에 비해 너무 작은 값이 될 수 있으므로, 결과에 스케일링 계수(lora_alpha/r)를 곱해서 크기를 조절해주는 것.\n","    lora_dropout=0.05, # -> 과적합 방지를 위한 드롭아웃 확률 설정\n","    bias=\"none\", # -> 기존 모델의 bias(편향) 파라미터를 학습에 포함시킬 지 여부 지정 (\"lora_only\", \"all\" 옵션도 존재함)\n","    task_type=\"CAUSAL_LM\", # -> 작업 종류(저장/로드/일부 내부 로직에서 적절한 타입을 )\n","    target_modules=target_modules # -> 어떤 선형층에 LoRA를 주입할지 선택\n",")\n","\n","  # 6-4) LoRA 모듈 주입\n","# model_ready의 지정된 target_modules 선형층마다 \"LoRA 어댑터(저랭크 A/B 행렬)\"가 붙는다\n","\n","  # 6-4) LoRA 모듈 주입\n","  # model_ready의 지정된 target_modules 선형층마다 \"LoRA 어댑터(저랭크 A/B 행렬)\"가 붙는다\n","model = get_peft_model(model_ready, peft_conf)\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"l7d_t_QDH-fO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. N 에폭만큼 학습 진행"],"metadata":{"id":"5Wf1EqqwfI7Q"}},{"cell_type":"code","source":[],"metadata":{"id":"vz3j7ojtfMbM"},"execution_count":null,"outputs":[]}]}