{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcdr+Y7OpkdX4e4WekeNPs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5e34239f67fd4ceb8ccfc4fe060164c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83507de6f992484c9ed8a1aedc831c7b","IPY_MODEL_06b8cdd99d8e46cc8edcef3e68e53648","IPY_MODEL_2a37ca2cc77546729e072ee0a081aaf0"],"layout":"IPY_MODEL_0e1a38ddd2b24e598b750e47741ef17e"}},"83507de6f992484c9ed8a1aedc831c7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65228a7d5e814a1c872cb83ffade2e8a","placeholder":"​","style":"IPY_MODEL_02a75eb31887495ba9280d1d76c287e0","value":"Map (num_proc=4): 100%"}},"06b8cdd99d8e46cc8edcef3e68e53648":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_364fecc74244428a907458038c194bfe","max":280,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01aaf73b76be4eb58e2ef7c0a1db876c","value":280}},"2a37ca2cc77546729e072ee0a081aaf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcc053830a3541c0901b4ad7f55ed3ba","placeholder":"​","style":"IPY_MODEL_f840bc40685b4eaaa13d825fd3444dae","value":" 280/280 [00:01&lt;00:00, 170.82 examples/s]"}},"0e1a38ddd2b24e598b750e47741ef17e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65228a7d5e814a1c872cb83ffade2e8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a75eb31887495ba9280d1d76c287e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"364fecc74244428a907458038c194bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01aaf73b76be4eb58e2ef7c0a1db876c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dcc053830a3541c0901b4ad7f55ed3ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f840bc40685b4eaaa13d825fd3444dae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02c75f1dc506465784e5dad048ef6a8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ad08a804d8d4b3d808868bbcd8d372e","IPY_MODEL_62010bb6f92447569f3af86fa9d28042","IPY_MODEL_a28cb6a7c9784d62b804abccaee456a3"],"layout":"IPY_MODEL_08dad1799c3c4aa7833922557490edd3"}},"6ad08a804d8d4b3d808868bbcd8d372e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6df121d51fe246b191b897be6b223db5","placeholder":"​","style":"IPY_MODEL_05e9c61753244de08b6873623fdb9ad4","value":"Map (num_proc=4): 100%"}},"62010bb6f92447569f3af86fa9d28042":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59d0ad9810784facabbf32939b6945db","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_939e9bc9644346a492dd986628a6d717","value":32}},"a28cb6a7c9784d62b804abccaee456a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ae1484aab504eaba03ca3a64274fc03","placeholder":"​","style":"IPY_MODEL_110168e9cbc44574915e48eaaa7012c4","value":" 32/32 [00:00&lt;00:00, 39.21 examples/s]"}},"08dad1799c3c4aa7833922557490edd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df121d51fe246b191b897be6b223db5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05e9c61753244de08b6873623fdb9ad4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59d0ad9810784facabbf32939b6945db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939e9bc9644346a492dd986628a6d717":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ae1484aab504eaba03ca3a64274fc03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"110168e9cbc44574915e48eaaa7012c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 0. 드라이브 - 코랩 연결"],"metadata":{"id":"db5RLF7jPjQE"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWv_qmB7MW4d","executionInfo":{"status":"ok","timestamp":1755590748505,"user_tz":-540,"elapsed":20001,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"d4930103-299a-43d8-e0eb-4b3dc478bbd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 1. 토크나이즈된 .arrow 데이터 불러오기"],"metadata":{"id":"Wxe3B0aXPqrg"}},{"cell_type":"code","source":["from datasets import load_from_disk\n","\n","train_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/train_tokenized')\n","valid_ds = load_from_disk('/content/drive/MyDrive/DILAB/HJ/News_DILAB/valid_tokenized')\n","\n","print(len(train_ds[\"input_ids\"]))\n","print(valid_ds)\n","\n","# \"input_ids\" : 토크나이저가 텍스트를 분해해 만든 토큰 정수 ID 배열\n","# \"attention_mask\" : 어떤 토큰을 실제로 계산에 반영할지 표시하는 마스크\n","  # ㄴ> 1: 유효 토큰, 0: 패딩(무시)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EY_tK9vyPrGw","executionInfo":{"status":"ok","timestamp":1755591138467,"user_tz":-540,"elapsed":124,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"f0d23b26-ca69-4ff1-c8d9-ee6003727f98"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["2048\n","Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 32\n","})\n"]}]},{"cell_type":"markdown","source":["# 2. labels(학습을 위한 정답지) 생성"],"metadata":{"id":"Y78F2P0PS0hY"}},{"cell_type":"code","source":["IGNORE_INDEX = -100 # CE Loss의 ignore_index 값은 -100이다.\n","\n","# 해당 train/valid 데이터셋에 각각의 labels를 추가해주는 함수\n","def add_labels_batched(batch):\n","  ids_batch = batch[\"input_ids\"]\n","  # attention_mask를 가져오는데, 만약 해당 값이 없다면 default 값으로 input_ids 길이만큼의 \"1\"로 채워진 batch형 배열 반환\n","  am_batch = batch.get(\"attention_mask\", [[1]*len(ids) for ids in ids_batch])\n","\n","  # attention_mask에서 1이 아닌 자리는 IGNORE_INDEX로 채우고, 1인 자리는 input_ids의 값을 복사한다.\n","  # 해당 작업을 배치 단위로 진행한다.\n","  labels_batch = [\n","      [tok if m==1 else IGNORE_INDEX for tok, m in zip(ids, am)]\n","      for ids, am in zip(ids_batch, am_batch)\n","  ]\n","  return {\"labels\": labels_batch}\n","\n","# 각각의 데이터셋에 \"labels\"를 추가함\n","  # num_proc : 병렬 처리에 사용할 프로세스 개수\n","  # batched : 해당 map작업을 배치 단위로 하는지 여부 설정\n","# ㄴ> map 함수는 해당 함수 작업을 수행한 결과를 해당 데이터셋에 추가한 값을 반환한다.\n","train_ds = train_ds.map(add_labels_batched, batched=True, num_proc=4)\n","valid_ds = valid_ds.map(add_labels_batched, batched=True, num_proc=4)\n","\n","\n","# 해당 데이터셋에는 \"text\" 컬럼이 있는데, 해당 컬럼은 학습 과정에서 필요 없기 때문에 제거한다\n","train_ds = train_ds.remove_columns(\"text\")\n","valid_ds = valid_ds.remove_columns(\"text\")"],"metadata":{"id":"Ml3mZa6tS0v4","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["5e34239f67fd4ceb8ccfc4fe060164c7","83507de6f992484c9ed8a1aedc831c7b","06b8cdd99d8e46cc8edcef3e68e53648","2a37ca2cc77546729e072ee0a081aaf0","0e1a38ddd2b24e598b750e47741ef17e","65228a7d5e814a1c872cb83ffade2e8a","02a75eb31887495ba9280d1d76c287e0","364fecc74244428a907458038c194bfe","01aaf73b76be4eb58e2ef7c0a1db876c","dcc053830a3541c0901b4ad7f55ed3ba","f840bc40685b4eaaa13d825fd3444dae","02c75f1dc506465784e5dad048ef6a8f","6ad08a804d8d4b3d808868bbcd8d372e","62010bb6f92447569f3af86fa9d28042","a28cb6a7c9784d62b804abccaee456a3","08dad1799c3c4aa7833922557490edd3","6df121d51fe246b191b897be6b223db5","05e9c61753244de08b6873623fdb9ad4","59d0ad9810784facabbf32939b6945db","939e9bc9644346a492dd986628a6d717","6ae1484aab504eaba03ca3a64274fc03","110168e9cbc44574915e48eaaa7012c4"]},"executionInfo":{"status":"ok","timestamp":1755591065234,"user_tz":-540,"elapsed":1656,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"6c625956-c616-4911-b510-e397c75f2133"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/280 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e34239f67fd4ceb8ccfc4fe060164c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/32 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c75f1dc506465784e5dad048ef6a8f"}},"metadata":{}}]},{"cell_type":"markdown","source":["# 3. 모델에 8비트 양자화 + LoRA 적용"],"metadata":{"id":"2df0b72w1RF-"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, default_data_collator\n","from transformers import AutoTokenizer  # 임베딩 리사이즈 & 생성 시에만 사용\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","model_path = \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n","# PAD 토큰이 없다면, EOS 토큰으로 해당 역할 대체\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","# 1. 8비트 양자화 로드\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit = True,\n","    # fp16 대신 bfloat16을 연산 타입으로 사용 (bfloat16이 fp16보다 안정적임)\n","    bnb_8bit_compute_dtype = torch.bfloat16\n",")\n","\n","# 2. 8비트 양자화 적용한 모델 로드\n","# AutoModelForCausalLM으로 8비트 양자화를 적용하여 저장해둔 모델(라마 3)을 로드한다.\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    quantization_config = bnb_config, # 모델을 8-bit 양자화 하여 GPU 메모리를 절약 (Hugging Face + bitsandbytes 라이브러리로 작동함)\n","    device_map = \"auto\", # \"auto\": 모델을 사용 가능한 장치에 자동으로 분산해서 올려준다 (GPU, CPU에 자동 분산) / \"cuda' : 자동 분산하지 않고 단일 GPU 사용\n","    torch_dtype = torch.bfloat16 # torch의 데이터 타입을 bfloat16으로 바꿔줌\n",")\n","\n","# 3.base_model의 입력 임베딩 테이블 resize\n","  # naver_news_tokeniziner 코드에서 텍스트에 \"<title></title>, <body></body>\" 스페셜 토큰을 적용해준 뒤 Tokenizer에도\n","  # 해당 스페셜 토큰들을 추가해주었었기 때문에 vocab(해당 Tokenizer의 어휘 사전 목록 길이)가 커졌다.\n","  # 따라서, 모델의 임베딩 테이블의 크기 또한 새로운 vocab으로 설정해주지 않으면 학습 중에 해당 토큰 id를 임베딩에서 찾을\n","  # 때 IndexError(범위 초과)가 날 수 있다.\n","    # ㄴ> 토크나이징 결과 숫자는 해당 토큰의 ID(Tokenizer, Model의 어휘 사전(vocab)에서의 위치)이다.\n","    # ㄴ> 해당 토큰 ID로 임베딩 테이블의 vector값을 찾는 것이다. (이 벡터값이 학습 중에 업데이트되는 가중치이다.)\n","    # ㄴ> 그렇기 때문에, resize_token_embedding()을 해주지 않으면, 임베딩 테이블에서 indexError가 날 수 있는 것이다.\n","base_model.resize_token_embeddings(len(tokenizer)) # base_model의 임베딩 행 개수를 늘려, 새 토큰들용 임베딩을 추가해야됨\n","  # ㄴ> model.resize_token_embeddings() : 모델의 입력 임베딩 테이블을 새 vocab 크기에 맞게 리사이즈하는 함수\n","    # 늘어나는 행 => 랜덤 초기화. 학습 중에 업데이트 된다\n","    # 줄이는 행 => 뒤쪽 행을 잘라냄\n","\n","# 4. K/V 캐시(past_key_values) 사용 여부를 끈다\n","  # 학습 중에는 gradient checkpointing과 충돌/경고가 나거나, 캐시 유지가 메모리를 더 먹을 수 있음\n","  # 따라서, 보통 학습할 때는 K/V 옵션을 끄며, 추론&생성 시에는 킨다\n","  # 효과 : 학습 중 경고 제거, 메모리 절약\n","base_model.config.use_cache = False\n","\n","# 5. Gradient(Activation) Checkpointing을 킨다\n"," # Gradient(Activation) Checkpointing : 순전파 때 중간 활성값을 전부 저장하지 않고, 역전파 때 일부를 재계산해서 VRAM을 크게 절약하는 기법\n"," # 장점: 메모리 사용량 감소 / 단점 : 연산량 증가\n","base_model.gradient_checkpointing_enable()\n","\n","\n","# 6. LoRA 적용\n","\n","  # 6-1) 양자화된 모델을 LoRA 학습이 가능한 상태로 설정\n","# prepare_model_for_kbit_training(model) : 8비트/4비트 양자화된 모델을 LoRA로 학습 가능한 상태로 안전하게 세팅해주는 함수\n","# LoRA를 모델에 붙이기 전에 해당 작업을 해주는 것이 바람직함\n","model_ready = prepare_model_for_kbit_training(base_model)\n","\n","  # 6-2) LoRA를 적용할 선형층 지정\n","# \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\" => 어텐션 모듈의 Q/K/V/O 투영 선형층\n","# \"gate_proj\",\"up_proj\",\"down_proj\" => FFN(MLP)의 게이트/상향/하향 선형층\n","target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]  # LLaMA3 계열 흔한 선택\n","\n","  # 6-3) LoRA 설정값 지정 (LoraConfig)\n","# LoraConfig() : LoRA 방식의 설정값들을 지정하는 함수\n","peft_conf = LoraConfig(\n","    r=16, # -> 랭크(학습할 저랭크 행렬의 차원 수). 클수록 표현력&파라미터 수 증가, VRAM/연산 증가 (보통 8~64 범위로 지정함)\n","    lora_alpha=32,  # -> scaling factor로, 최종 결과는 \"lora_alpha / r\"로 스케일링 됨(모델 성능에 영향을 줄 수 있음)\n","                    # -> 앞서 만든 \"A @ B\" 저랭크 행렬이 원래 weight W에 비해 너무 작은 값이 될 수 있으므로, 결과에 스케일링 계수(lora_alpha/r)를 곱해서 크기를 조절해주는 것.\n","    lora_dropout=0.05, # -> 과적합 방지를 위한 드롭아웃 확률 설정\n","    bias=\"none\", # -> 기존 모델의 bias(편향) 파라미터를 학습에 포함시킬 지 여부 지정 (\"lora_only\", \"all\" 옵션도 존재함)\n","    task_type=\"CAUSAL_LM\", # -> 작업 종류(저장/로드/일부 내부 로직에서 적절한 타입을 )\n","    target_modules=target_modules # -> 어떤 선형층에 LoRA를 주입할지 선택\n",")\n","\n","  # 6-4) LoRA 모듈 주입\n","# model_ready의 지정된 target_modules 선형층마다 \"LoRA 어댑터(저랭크 A/B 행렬)\"가 붙는다\n","# get_peft_model() : LoRA 설정에 맞게 기존 모델의 일부 모듈(q_proj, v_proj 등)에 LoRA 계층을 삽입해서, 전체 모델을 래핑(wrap)해준다.\n","model = get_peft_model(model_ready, peft_conf)\n"],"metadata":{"id":"l7d_t_QDH-fO","colab":{"base_uri":"https://localhost:8080/","height":628},"executionInfo":{"status":"error","timestamp":1755591091992,"user_tz":-540,"elapsed":22694,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"33ec377c-ca25-4a14-d10b-eca937c55eee"},"execution_count":26,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1360613874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 2. 8비트 양자화 적용한 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# AutoModelForCausalLM으로 8비트 양자화를 적용하여 저장해둔 모델(라마 3)을 로드한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mquantization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 모델을 8-bit 양자화 하여 GPU 메모리를 절약 (Hugging Face + bitsandbytes 라이브러리로 작동함)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4887\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4888\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4889\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_library_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# 4. N 에폭만큼 학습 진행"],"metadata":{"id":"5Wf1EqqwfI7Q"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","import torch.nn.utils as nn_utils\n","from contextlib import nullcontext\n","from tqdm.auto import tqdm\n","import math\n","\n","# 1. 배치 단위로 데이터 공급해주기 위해 DataLoader 사용\n","def collate_fn(batch):\n","    return {\n","        \"input_ids\":      torch.tensor([b[\"input_ids\"]      for b in batch], dtype=torch.long),\n","        \"attention_mask\": torch.tensor([b[\"attention_mask\"] for b in batch], dtype=torch.long),\n","        \"labels\":         torch.tensor([b[\"labels\"]         for b in batch], dtype=torch.long),\n","    }\n","\n","# 미니 배치들을 만드는 과정이다. 한 번의 iteration마다 샘플을 batch_size만큼 개를 꺼내온다는 뜻\n","# 데이터셋의 각 컬럼은 [X * Y]처럼 생겼는데, 해당 데이터에서 batch_size만큼의 행(샘플)들을 꺼내와서 배치화한다는 것이다.\n","train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  collate_fn=collate_fn, pin_memory=True)\n","valid_loader = DataLoader(valid_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n","\n","\n","\n","# 2. Optimizer 설정(Peft 사용 시, base_model의 trainable 파라미터만 학습된다.)\n","  # 옵티마이저는 기울기(미분값)를 사용하여 파라미터를 업데이트하는 역할을 한다.\n","\n","# optimizer = AdamW(model.parameters(), lr=2e-4) # -> Full 파인튜닝(모든 가중치 학습)을 할 때\n","  # ㄴ> Pytorch의 AdamW는 grad가 없는 파라미터는 step()에서 건너뛰지만, 불필요한 파라미터 레퍼런스를 옵티마이저가 들고 있게 되므로, 오버헤드가 증가하게 된다.\n","\n","optimizer = AdamW([p if p.requires_grad for p in model.parameters()], lr-2e-4)\n","  # ㄴ> LoRA는 본체 가중치는 동결하고 LoRA 어댑터만 학습하기 때문에, 현재 model의 파라미터 중 학습할 파라미터만 모아서 옵티마이저에게 넘겨주어야 한다\n","  # ㄴ> requires_grad 속성으로 해당 파라미터가 학습할 파라미터인지 아닌지 확인한다.\n","\n","\n","# 3. GPU 사용 준비\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# 4. bf16 autocast (지원 GPU면 on하도록 하는 설정)\n","use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8  # Ampere+\n","amp_ctx = (torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16) if use_bf16 else nullcontext())\n","  # ㄴ> torch.autocast() 함수가 return한 객체는 \"컨텍스트 매니저 객체\"이다.\n","  # ㄴ> with와 함께 사용 시, 블록 내 연산을 \"자동 혼합 정밀(AMP)\"로 돌려서 속도&VRAM을 절약할 수 있다,\n","  # ㄴ> autocast 미지원 시, \"nullcontext()\"로 아무 것도 하지 않는 컨텍스트(fp32)로 동작한다.\n","\n","\n","# 5. 에폭 설정\n","epochs = 2\n","grad_accum = 8  # 몇개의 미니 배치가 모이면 optimizer로 한번에 기울기 업데이트를 할지 정하는 변수\n","\n","\n","# 6. 학습 시작\n","global_step = 0\n","for epoch in range(epochs):\n","  # 해당 모델을 \"학습 모드\"로 설정\n","  model.train()\n","\n","  total_loss = 0\n","\n","  # tqdm : 진행 상황을 bar로 보여주는 라이브러리\n","  running = 0.0\n","  loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\") # 데이터를 batch 단위로 반환해주는 Dataloader로 설정한 train_loader를 설정해준다.\n","        #  ㄴ> tqdm은 원래 iterable인 DataLoader를 감싼 iterable 객체이므로, \"for batch in tqdm_obj\"로 처럼 batch를 꺼내올 수 있다.\n","\n","  optimizer.zero_grad(set_to_none=True)\n","\n","  # DataLoader가 미니 배치로 나누어서 데이터들을 준다.\n","  # GPU의 한계 때문에 모든 데이터를 한번에 올리지 않고, 여러 개의 미니 배치로 데이터를 나눈 후 gradient 누적을 통해\n","  # 모든 데이터를 한번에 계산한 것과 같은(비슷한) 효과를 내도록 하는 것\n","    # grad_accum은 몇개의 배치를 묶어서 한 번 업데이트할 지를 지정하는 변수\n","  for step, batch in enumerate(loop, start=1): # enumerate는 매 반복마다 (인덱스, 값) 쌍을 뱉기 때문에 step, batch로 모은다\n","      # ㄴ> step : 만들어질 수 있는 총 배치의 개수\n","      batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n","        # ㄴ> 딕셔너리 컴프리헨션 -> {키식: 값식 for k, v in ...}\n","        # ㄴ> 기존 batch 딕셔너리의 모든 entry 쌍을 돌면서 값을 device(GPU)로 이동시킨다.\n","\n","      # autocast가 return한 AMP 컨텍스트 객체로 아래 블록의 작업을 수행한다.\n","      with amp_ctx:\n","          out = model(**batch)           # CausalLM: loss 포함\n","                  # ㄴ> 딕셔너리 언패킹 : batch 내부의 여러 컬럼들\n","                  # {\"input_ids\" : X, \"attention_mask\" : Y, \"labels\" : Z}를\n","                  # model(input_ids=X, attention_mask=Y, labels=Z)로 키워드 인자로 풀어 넣는다.\n","          loss = out.loss / grad_accum\n","            # ㄴ> 그라디언트 누적을 위해, 각 배치의 loss값을 grad_accum으로 나눈 뒤에, 해당 loss값을 이용하여 역전파를 수행한다\n","            # ㄴ> loss값을 grad_accum으로 나누는 이유는 N개의 미니 배치의 평균 기울기를 만들기 위해서이다.\n","\n","      loss.backward()\n","        # ㄴ> .backward()\n","\n","      if step % grad_accum == 0:\n","          nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","          optimizer.step()\n","          optimizer.zero_grad(set_to_none=True)\n","          global_step += 1\n","\n","      running += loss.item() * grad_accum\n","      if step % grad_accum == 0:\n","          loop.set_postfix(loss=running/grad_accum)\n","          running = 0.0\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"vz3j7ojtfMbM"},"execution_count":null,"outputs":[]}]}