{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOrzhEjHs99UJlhLmvtM7yj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"91aebd32584e494e8c8cf2977332bb81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91f991d1ad9440ab8b1cb9d039be530f","IPY_MODEL_97a60cd616214526a13c86a6574d0e76","IPY_MODEL_c09387e067b949d180f4f0b61b2e6c17"],"layout":"IPY_MODEL_fd5f2dfefe464199b89814ee952bb60b"}},"91f991d1ad9440ab8b1cb9d039be530f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_651cc6650a7d4b8fac73089060d21943","placeholder":"​","style":"IPY_MODEL_b30c0da3945e4a6da5bd41075a98720c","value":"Loading checkpoint shards: 100%"}},"97a60cd616214526a13c86a6574d0e76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46061c9186be469cbc77954048ca0812","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_877902a3fd4b4dea8ecacfc61445040b","value":4}},"c09387e067b949d180f4f0b61b2e6c17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b2baf0ea68f437590a6f94ecfc0b2cf","placeholder":"​","style":"IPY_MODEL_e0580c7ee79d463a9ff2f2eb420fde41","value":" 4/4 [06:52&lt;00:00, 86.91s/it]"}},"fd5f2dfefe464199b89814ee952bb60b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"651cc6650a7d4b8fac73089060d21943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b30c0da3945e4a6da5bd41075a98720c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46061c9186be469cbc77954048ca0812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877902a3fd4b4dea8ecacfc61445040b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b2baf0ea68f437590a6f94ecfc0b2cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0580c7ee79d463a9ff2f2eb420fde41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 0. 코랩 <-> 드라이브 연결"],"metadata":{"id":"82wBfHFhooPy"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYO3ZHMioy__","executionInfo":{"status":"ok","timestamp":1756116700129,"user_tz":-540,"elapsed":29937,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"2278b7b9-86a4-42f0-cb8c-b151392f84b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"VoUKKh0Wo0gZ"}},{"cell_type":"code","source":["!pip install -q transformers accelerate\n","!pip install -U bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5l_-IIaDo16T","executionInfo":{"status":"ok","timestamp":1756116715075,"user_tz":-540,"elapsed":14943,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"ce7fb00a-015c-46a8-d924-f642c470d745"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.47.0\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n","import textwrap\n","from peft import PeftModel\n","\n","# 1. 지정한 경로 가져옴\n","model_path = \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B\"\n","\n","# 2. 지정한 경로에서 토크나이저, Model들을 불러온다\n","  # 학습할 때 진행했던 토크나이저, 양자화, LoRA 설정 똑같이 설정\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n","# PAD 토큰이 없다면, EOS 토큰으로 해당 역할 대체\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","# 3. 모델 불러오기\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    device_map = \"auto\", # GPU 자동 할당\n","    torch_dtype = \"auto\" # fp16 등 자동 감지\n",")\n","\n","\n","# 4. K/V 캐시(past_key_values) 사용 설정\n","  # 학습 중에는 gradient checkpointing과 충돌/경고가 나거나, 캐시 유지가 메모리를 더 먹을 수 있음\n","  # 따라서, 보통 학습할 때는 K/V 옵션을 끄며, 추론&생성 시에는 킨다\n","model.config.use_cache = True\n","\n","# 5. **임베딩 크기 일치화**: 토크나이저 길이에 맞춰 리사이즈(← 이 것이 핵심!)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","\n","# 파이프라인 생성\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",     # GPU 할당\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["91aebd32584e494e8c8cf2977332bb81","91f991d1ad9440ab8b1cb9d039be530f","97a60cd616214526a13c86a6574d0e76","c09387e067b949d180f4f0b61b2e6c17","fd5f2dfefe464199b89814ee952bb60b","651cc6650a7d4b8fac73089060d21943","b30c0da3945e4a6da5bd41075a98720c","46061c9186be469cbc77954048ca0812","877902a3fd4b4dea8ecacfc61445040b","9b2baf0ea68f437590a6f94ecfc0b2cf","e0580c7ee79d463a9ff2f2eb420fde41"]},"id":"rFJ6T1RCo3Hx","executionInfo":{"status":"ok","timestamp":1756117159579,"user_tz":-540,"elapsed":444502,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"845a6f91-48e2-4ca3-9e66-260e3c0cce01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91aebd32584e494e8c8cf2977332bb81"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","Device set to use cuda:0\n"]}]},{"cell_type":"code","source":["\n","def generate_response(user_input, history, temperature=0.7, top_p=0.9, max_tokens=500):\n","  global pipe\n","  global his\n","  his = history\n","  print(history)\n","  messages = [{\"role\":\"assistant\", \"content\":\"너는 뛰어난 AI 어시스턴트야. 너는 상대방과 대화를 하는 챗봇이야.\"}]\n","\n","  for h in history:\n","    messages.append({\"role\":\"user\", \"content\": h[0]})\n","    messages.append({\"role\":\"assistant\", \"content\": h[1]})\n","\n","  messages.append({\"role\":\"user\", \"content\": user_input})\n","\n","  prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","  # 텍스트 생성\n","  output = pipe(\n","      prompt,\n","      max_new_tokens=max_tokens,     # 생성할 최대 토큰 수\n","      do_sample=True,        # 샘플링 방식 사용 (더 자연스럽게)\n","      temperature=temperature,       # 창의성 조절 (낮을수록 보수적)\n","      top_p=top_p,              # nucleus sampling\n","      eos_token_id=tokenizer.eos_token_id,  # 문장이 끝났음을 뜻하는 eos_token_id를 명시하여, 모델이 끝을 알게 하기\n","      return_full_text=False\n","  )\n","  # 출력 결과 확인\n","  output_text = output[0][\"generated_text\"]\n","\n","  #wrapped_text = textwrap.fill(output_text, width=70)\n","  return output_text\n"],"metadata":{"id":"_l9xlX7Uo5Gc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","his = [] # 대화 기록 저장용 리스트\n","\n","iface = gr.ChatInterface(\n","    fn=generate_response,\n","    title=\"Llama3 ChatBot\",\n","    description = \"Let's talking with me\"\n",")\n","iface.launch(share = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651},"id":"N2uemlAdo6a_","executionInfo":{"status":"ok","timestamp":1756117166059,"user_tz":-540,"elapsed":6466,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"90272653-4e43-4f69-b9a8-5078c9eb1667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://73102f25dde8afa470.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://73102f25dde8afa470.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]}]}