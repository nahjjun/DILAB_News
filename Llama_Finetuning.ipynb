{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOv4NGCLDr3QUtwzgnlOOsN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"76e63f89d94f41efb82ea5351cb4bf5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_427ada1dd2b844028d5362048740ad53","IPY_MODEL_671579a363a04de2bf9bce57141651d7","IPY_MODEL_62880d229f144429b73969f1814e2eec"],"layout":"IPY_MODEL_2232be41b1314e70a23d3cffdbb88b0e"}},"427ada1dd2b844028d5362048740ad53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bae5e5fd5d34712956503105332b39d","placeholder":"​","style":"IPY_MODEL_40d7caab03c54e9a91bb686da11eaf47","value":"Loading checkpoint shards: 100%"}},"671579a363a04de2bf9bce57141651d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a0b7b4d725495fafe76c68585b5cde","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae06e7cb3a484be6897f16b58c4a1c30","value":4}},"62880d229f144429b73969f1814e2eec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51763fd70cb641a0a21f36cd059585ed","placeholder":"​","style":"IPY_MODEL_a8a5833e1bd44690b52c832f446c4b93","value":" 4/4 [05:53&lt;00:00, 75.07s/it]"}},"2232be41b1314e70a23d3cffdbb88b0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bae5e5fd5d34712956503105332b39d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40d7caab03c54e9a91bb686da11eaf47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a0b7b4d725495fafe76c68585b5cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae06e7cb3a484be6897f16b58c4a1c30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51763fd70cb641a0a21f36cd059585ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8a5833e1bd44690b52c832f446c4b93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 0. 코랩 <-> 드라이브 연결"],"metadata":{"id":"5e0DDTgyY1CX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4AsjQml8YD","executionInfo":{"status":"ok","timestamp":1756114727458,"user_tz":-540,"elapsed":37840,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"324433e6-9d52-4f8b-f6ad-c574557232f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 1. 필요 라이브러리 다운로드"],"metadata":{"id":"PXanEhq4YyDA"}},{"cell_type":"code","source":["!pip install -q transformers accelerate\n","!pip install -U bitsandbytes"],"metadata":{"id":"z9XuUsSBmBJE","executionInfo":{"status":"ok","timestamp":1756114741460,"user_tz":-540,"elapsed":14000,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34666f4b-9b4f-490f-e525-2952fcc574a0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.47.0\n"]}]},{"cell_type":"markdown","source":["# 2-1. 지정한 경로에서 Model, Tokenizer 가져오기 & 파이프라인 생성 - 파인튜닝된 버전"],"metadata":{"id":"A53bh02TY7mT"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n","import textwrap\n","from peft import PeftModel\n","\n","# 1. 지정한 경로 가져옴\n","model_path = \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B\"\n","\n","# 2. 지정한 경로에서 토크나이저, Model들을 불러온다\n","  # 학습할 때 진행했던 토크나이저, 양자화, LoRA 설정 똑같이 설정\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n","# PAD 토큰이 없다면, EOS 토큰으로 해당 역할 대체\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","# 3. 양자화 설정\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit = True,\n","    # fp16 대신 bfloat16을 연산 타입으로 사용 (bfloat16이 fp16보다 안정적임)\n","    bnb_8bit_compute_dtype = torch.bfloat16\n",")\n","\n","# 4. 모델 불러오기\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    quantization_config=bnb_config, # 양자화 설정\n","    device_map = \"auto\", # GPU 자동 할당\n","    torch_dtype = \"auto\" # fp16 등 자동 감지\n",")\n","\n","\n","# 5. K/V 캐시(past_key_values) 사용 설정\n","  # 학습 중에는 gradient checkpointing과 충돌/경고가 나거나, 캐시 유지가 메모리를 더 먹을 수 있음\n","  # 따라서, 보통 학습할 때는 K/V 옵션을 끄며, 추론&생성 시에는 킨다\n","base_model.config.use_cache = True\n","\n","# 6. **임베딩 크기 일치화**: 토크나이저 길이에 맞춰 리사이즈(← 이것이 핵심!)\n","base_model.resize_token_embeddings(len(tokenizer))\n","\n","# 7. 최종 모델 불러오기\n","  # base_model에 LoRA Adapter 붙여서 사용해야함\n","model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/DILAB/llama3-Korean-Bllossom-8B/lora_adapter\")\n","\n","\n","# 파이프라인 생성\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",     # GPU 할당\n",")"],"metadata":{"id":"L-qfUh9eZAbs","executionInfo":{"status":"ok","timestamp":1756115241614,"user_tz":-540,"elapsed":500153,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["76e63f89d94f41efb82ea5351cb4bf5d","427ada1dd2b844028d5362048740ad53","671579a363a04de2bf9bce57141651d7","62880d229f144429b73969f1814e2eec","2232be41b1314e70a23d3cffdbb88b0e","4bae5e5fd5d34712956503105332b39d","40d7caab03c54e9a91bb686da11eaf47","73a0b7b4d725495fafe76c68585b5cde","ae06e7cb3a484be6897f16b58c4a1c30","51763fd70cb641a0a21f36cd059585ed","a8a5833e1bd44690b52c832f446c4b93"]},"outputId":"b1215586-163a-4397-d118-d22f881efafa"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e63f89d94f41efb82ea5351cb4bf5d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","Device set to use cuda:0\n"]}]},{"cell_type":"markdown","source":["# 3. 반환값 생성 함수 정의"],"metadata":{"id":"W3ihFZKOZCYX"}},{"cell_type":"code","source":["\n","def generate_response(user_input, history, temperature=0.7, top_p=0.9, max_tokens=500):\n","  global pipe\n","  global his\n","  his = history\n","  print(history)\n","  messages = [{\"role\":\"assistant\", \"content\":\"너는 뛰어난 AI 어시스턴트야. 너는 상대방과 대화를 하는 챗봇이야.\"}]\n","\n","  for h in history:\n","    messages.append({\"role\":\"user\", \"content\": h[0]})\n","    messages.append({\"role\":\"assistant\", \"content\": h[1]})\n","\n","  messages.append({\"role\":\"user\", \"content\": user_input})\n","\n","  prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","  # 텍스트 생성\n","  output = pipe(\n","      prompt,\n","      max_new_tokens=max_tokens,     # 생성할 최대 토큰 수\n","      do_sample=True,        # 샘플링 방식 사용 (더 자연스럽게)\n","      temperature=temperature,       # 창의성 조절 (낮을수록 보수적)\n","      top_p=top_p,              # nucleus sampling\n","      eos_token_id=tokenizer.eos_token_id,  # 문장이 끝났음을 뜻하는 eos_token_id를 명시하여, 모델이 끝을 알게 하기\n","      return_full_text=False\n","  )\n","  # 출력 결과 확인\n","  output_text = output[0][\"generated_text\"]\n","\n","  #wrapped_text = textwrap.fill(output_text, width=70)\n","  return output_text\n"],"metadata":{"id":"xtRALkRMZEUS","executionInfo":{"status":"ok","timestamp":1756115241619,"user_tz":-540,"elapsed":2,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 4. Gradio 웹 UI 인터페이스 구축"],"metadata":{"id":"aXRCerjoZE7Y"}},{"cell_type":"code","source":["import gradio as gr\n","\n","his = [] # 대화 기록 저장용 리스트\n","\n","iface = gr.ChatInterface(\n","    fn=generate_response,\n","    title=\"Llama3 ChatBot\",\n","    description = \"Let's talking with me\"\n",")\n","\n","iface.launch(share = True)"],"metadata":{"id":"XhZTVjXpZKPt","executionInfo":{"status":"ok","timestamp":1756115247619,"user_tz":-540,"elapsed":5999,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"colab":{"base_uri":"https://localhost:8080/","height":651},"outputId":"64326e81-f75c-4f4c-fda3-51036245217c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://2cd7f95c286834bb27.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://2cd7f95c286834bb27.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]}]}